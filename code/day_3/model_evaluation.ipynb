{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d70026",
   "metadata": {},
   "source": [
    "# 1) Laden der Daten und trainierten Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7bb16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a784ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_parquet(\"../../data/day_3/X_test.parquet\")\n",
    "y_test = pd.read_parquet(\"../../data/day_3/y_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../models/random_forest_model.pkl\", \"rb\") as f:\n",
    "    base_rfc = pickle.load(f)\n",
    "\n",
    "with open(\"../../models/tuned_rfc_model.pkl\", \"rb\") as f:\n",
    "    tuned_rfc = pickle.load(f)\n",
    "\n",
    "with open(\"../../models/KNN_model.pkl\", \"rb\") as f:\n",
    "    base_knn = pickle.load(f)\n",
    "\n",
    "with open(\"../../models/tuned_knn_model.pkl\", \"rb\") as f:\n",
    "    tuned_knn = pickle.load(f)\n",
    "\n",
    "with open(\"../../models/SVC_model.pkl\", \"rb\") as f:\n",
    "    base_svc = pickle.load(f)\n",
    "\n",
    "with open(\"../../models/tuned_svc_model.pkl\", \"rb\") as f:\n",
    "    tuned_svc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\"base_rfc\": base_rfc, \"tuned_rfc\": tuned_rfc, \"base_knn\": base_knn,\n",
    "          \"tuned_knn\": tuned_knn, \"base_svc\": base_svc, \"tuned_svc\": tuned_svc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57023292",
   "metadata": {},
   "source": [
    "# 2) Vorhersage der Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52697793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X_test):\n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for model_name, model in MODELS.items():\n",
    "    print(f\"Predicting with {model_name}...\")\n",
    "    y_pred = predict(model, X_test)\n",
    "    predictions[model_name] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f85fc",
   "metadata": {},
   "source": [
    "### Abspeichern der Vorhersagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a34262",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df['groundtruth'] = y_test['Weather Type'].values\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79093613",
   "metadata": {},
   "source": [
    "# 3) Visualisieren der Vorhersagen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65aff4",
   "metadata": {},
   "source": [
    "## 3.1) Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a3788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_confusion_matrix(model_name, y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(f'Confusion Matrix des Modells {model_name}')\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Vorhergesagt')\n",
    "    plt.ylabel('Tatsächlich')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb54cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41babc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, y_pred in predictions.items():\n",
    "    create_confusion_matrix(model_name, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652cd5e1",
   "metadata": {},
   "source": [
    "## 3.2) Precision Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe268c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_precision_recall_curve(model_name, model, target_classes):\n",
    "    y_test_bin = label_binarize(y_test, classes=range(len(target_classes)))\n",
    "    y_score = model.predict_proba(X_test)\n",
    "\n",
    "    # Plot PR curve for each class\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    for i, color in zip(range(4), colors):\n",
    "        precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        ap = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "        plt.plot(recall, precision, color=color, lw=2,\n",
    "                label=f\"{target_classes[i]} (AP = {ap:.2f})\")\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision-Recall Curve (One-vs-Rest) for model {model_name}\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/day_3/label_encoders.pkl', 'rb') as f:\n",
    "    encoders_loaded = pickle.load(f)\n",
    "\n",
    "encoders_loaded[\"Weather Type\"].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126fa32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, trained_model in MODELS.items():\n",
    "    try:\n",
    "        plot_precision_recall_curve(model_name, trained_model, list(encoders_loaded['Weather Type'].classes_))\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting precision-recall curve for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406565db",
   "metadata": {},
   "source": [
    "## 3.3) ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ce542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_auc_curve(model_name, model, target_classes):\n",
    "    # Binarize the true labels\n",
    "    y_test_bin = label_binarize(y_test, classes=range(len(target_classes)))\n",
    "    y_score = model.predict_proba(X_test)\n",
    "\n",
    "    # Plot ROC curve for each class\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    for i, color in zip(range(len(target_classes)), colors):\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=color, lw=2,\n",
    "                label=f\"{target_classes[i]} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "    # Add diagonal reference line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "    # Formatting\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve (One-vs-Rest) for model {model_name}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f115fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, trained_model in MODELS.items():\n",
    "    try:\n",
    "        plot_roc_auc_curve(model_name, trained_model, list(encoders_loaded['Weather Type'].classes_))\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting precision-recall curve for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc689f0",
   "metadata": {},
   "source": [
    "## 3.3) ROC vs. PR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff31776",
   "metadata": {},
   "source": [
    "### ROC:\n",
    "zeigt, wie gut das Modell zwischen den Klassen unterscheiden kann.\n",
    "\n",
    "Schlechter bei unausgewogener Klassenverteilung, da FPR verfälscht sein kann, wenn es viele TN gibt\n",
    "\n",
    "Ein Modell, das zufällig rät, ergibt eine diagonale Linie von (0,0) bis (1,1).\n",
    "\n",
    "Die AUC (Area Under Curve – Fläche unter der Kurve) dient als zusammenfassender Einzelwert:\n",
    "\n",
    "1,0 = perfekter Klassifikator\n",
    "\n",
    "0,5 = nicht besser als Zufall\n",
    "\n",
    "### Precision-Recall-Kurve\n",
    "konzentriert sich ausschließlich auf die positive Klasse.\n",
    "\n",
    "Sie ist besonders nützlich, wenn die positive Klasse selten ist (z. B. bei Betrugserkennung oder Krankheitsdiagnosen).\n",
    "--> Besser bei unausgewogener Klassenverteilung\n",
    "\n",
    "Ein Modell, das zufällig rät, ergibt eine horizontale Linie auf Höhe der Häufigkeit der positiven Klasse.\n",
    "\n",
    "Die Average Precision (AP) ist die Fläche unter dieser Kurve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157be35",
   "metadata": {},
   "source": [
    "Wann was verwenden ?\n",
    "\n",
    "| Situation                                      | ROC-Kurve ✅ | Precision-Recall-Kurve ✅ |\n",
    "|-----------------------------------------------|--------------|----------------------------|\n",
    "| **Ausgewogene Klassenverteilung**             | ✅ Gut geeignet | ✅ Gut geeignet           |\n",
    "| **Unausgewogene Klassenverteilung**           | ❌ Irreführend möglich | ✅ Besser geeignet      |\n",
    "| Ziel: **Allgemeine Modellqualität bewerten**  | ✅ Ja         | ❌ Weniger aussagekräftig |\n",
    "| Ziel: **Leistung bei positiver Klasse bewerten** | ❌ Ungeeignet | ✅ Sehr gut geeignet     |\n",
    "| Fokus auf **True Positive Rate (Empfindlichkeit)** | ✅ Ja         | ✅ Ja                     |\n",
    "| Fokus auf **Präzision bei positiven Vorhersagen** | ❌ Nicht direkt | ✅ Optimal                |\n",
    "| Zusammenfassender Kennwert                    | AUC (Area Under Curve) | AP (Average Precision)   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26ceda",
   "metadata": {},
   "source": [
    "# 4) Berechnen der Performance der Vorhersagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18dcafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for prediction in predictions_df.columns:\n",
    "    if prediction != \"groundtruth\":\n",
    "        print(f\"Classification Accuracy in % for {prediction}:\")\n",
    "        print(str(round(100 * accuracy_score(y_test, predictions_df[prediction]), 3)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "for prediction in predictions_df.columns:\n",
    "    if prediction != \"groundtruth\":\n",
    "        print(f\"Classification Precision (Macro) in % for {prediction}:\")\n",
    "        print(str(round(100 * precision_score(y_test, predictions_df[prediction], average=\"macro\"), 3)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2efe812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for prediction in predictions_df.columns:\n",
    "    if prediction != \"groundtruth\":\n",
    "        print(f\"Classification Precision (Macro) in % for {prediction}:\")\n",
    "        print(str(round(100 * f1_score(y_test, predictions_df[prediction], average=\"macro\"), 3)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for prediction in predictions_df.columns:\n",
    "    if prediction != \"groundtruth\":\n",
    "        print(f\"Classification Report for {prediction}:\")\n",
    "        print(classification_report(y_test, predictions_df[prediction], target_names=encoders_loaded[\"Weather Type\"].classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probas(model, X_test):\n",
    "    return model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cba934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for model_name, model in MODELS.items():\n",
    "    print(f\"Predicting with {model_name}...\")\n",
    "    y_pred = predict_probas(model, X_test)\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "    print(\"ROC AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8583a40",
   "metadata": {},
   "source": [
    "# 5) Berechnen der Performance mit Kreuzvalidierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd9afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(base_rfc, X_test, np.ravel(y_test,), cv=cv, scoring=\"f1_macro\")\n",
    "print(\"F1-Score (5-fold CV):\", cv_scores)\n",
    "print(\"Durchschnittlicher F1 des base_rfc:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(tuned_svc, X_test, np.ravel(y_test,), cv=cv, scoring=\"f1_macro\")\n",
    "print(\"F1-Score (5-fold CV):\", cv_scores)\n",
    "print(\"Durchschnittlicher F1 des base_svc:\", np.mean(cv_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
