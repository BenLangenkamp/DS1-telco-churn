{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelltraining f√ºr Telco Customer Churn\n",
    "\n",
    "Dieses Notebook f√ºhrt das komplette Modelltraining durch:\n",
    "1. Daten laden und vorbereiten\n",
    "2. Features ausw√§hlen und encodieren\n",
    "3. Train/Test Split\n",
    "4. Mehrere Modelle trainieren\n",
    "5. Evaluation und Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n",
    "                             roc_auc_score, roc_curve)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot-Einstellungen\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úì Alle Libraries importiert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Schritt 1: Daten Laden\n",
    "\n",
    "Wir laden den Telco Customer Churn Datensatz. \n",
    "\n",
    "**Passe den Pfad an deinen Datensatz an!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 1: DATEN LADEN\n",
    "# ============================================================================\n",
    "# Hier laden wir den Telco Datensatz.\n",
    "# WICHTIG: Passe den Pfad an, falls deine Datei woanders liegt!\n",
    "# ============================================================================\n",
    "\n",
    "# === PFAD ANPASSEN! ===\n",
    "data = pd.read_csv('../../data/day_3/telco-customer-churn/train.csv')\n",
    "\n",
    "print(f\"Datensatz geladen: {data.shape[0]} Zeilen, {data.shape[1]} Spalten\")\n",
    "print(f\"\\nSpalten:\\n{data.columns.tolist()}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 1b: ZIELVARIABLE PR√úFEN\n",
    "# ============================================================================\n",
    "# Die Zielvariable ist 'Churn' - das ist was wir vorhersagen wollen.\n",
    "# Wir pr√ºfen:\n",
    "#   - Welche Werte gibt es? (0/1 oder Yes/No?)\n",
    "#   - Wie ist die Verteilung? (Wie viele Kunden sind abgewandert?)\n",
    "#\n",
    "# Eine unbalancierte Verteilung (z.B. 90% No Churn, 10% Churn) kann \n",
    "# das Training erschweren!\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ZIELVARIABLE: Churn\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nWerte: {data['Churn'].unique()}\")\n",
    "print(f\"\\nVerteilung:\")\n",
    "print(data['Churn'].value_counts())\n",
    "print(f\"\\nProzentual:\")\n",
    "print((data['Churn'].value_counts(normalize=True) * 100).round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Schritt 2: Features Ausw√§hlen\n",
    "\n",
    "Nicht alle Spalten sind n√ºtzlich f√ºr die Vorhersage:\n",
    "- **Customer ID**: Nur eine ID, kein Vorhersagewert\n",
    "- **Churn**: Das ist unsere ZIELVARIABLE (was wir vorhersagen wollen)\n",
    "- **Churn Category/Reason**: Das wissen wir nur NACHDEM jemand churned ‚Üí Data Leakage!\n",
    "- **City, State, Zip Code, Lat/Long**: Zu granular, w√ºrde Overfitting verursachen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 2: FEATURES AUSW√ÑHLEN\n",
    "# ============================================================================\n",
    "# Wir entfernen Spalten die:\n",
    "#   1. Keine Vorhersagekraft haben (IDs)\n",
    "#   2. Data Leakage verursachen (Info die wir nur nach Churn wissen)\n",
    "#   3. Zu granular sind (jeder Kunde hat eigenen Wert ‚Üí Overfitting)\n",
    "#\n",
    "# X = Features (alle Eingabe-Variablen)\n",
    "# y = Target (was wir vorhersagen wollen = Churn)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SCHRITT 2: FEATURES AUSW√ÑHLEN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Spalten die wir NICHT als Features wollen\n",
    "drop_cols = [\n",
    "    'Customer ID',           # Nur eine ID, kein Vorhersagewert\n",
    "    'Churn',                 # Zielvariable (nicht als Feature!)\n",
    "    'Churn Category',        # Data Leakage - wissen wir nur nach Churn\n",
    "    'Churn Reason',          # Data Leakage - wissen wir nur nach Churn\n",
    "    'Customer Status',       # Data Leakage - h√§ngt direkt mit Churn zusammen\n",
    "    'City',                  # Zu granular (zu viele einzigartige Werte)\n",
    "    'State',                 # K√∂nnten wir behalten, aber vereinfachen wir\n",
    "    'Country',               # Nur ein Wert (United States) - nutzlos\n",
    "    'Zip Code',              # Zu granular\n",
    "    'Lat Long',              # Redundant mit Latitude/Longitude\n",
    "    'Latitude',              # Geografische Daten, nicht relevant\n",
    "    'Longitude'              # Geografische Daten, nicht relevant\n",
    "]\n",
    "\n",
    "# Nur Spalten droppen die tats√§chlich existieren\n",
    "drop_cols = [c for c in drop_cols if c in data.columns]\n",
    "print(f\"\\nEntfernte Spalten ({len(drop_cols)}):\")\n",
    "for col in drop_cols:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "# Features und Target trennen\n",
    "X = data.drop(columns=drop_cols)\n",
    "y = data['Churn']\n",
    "\n",
    "print(f\"\\n‚úì Features (X): {X.shape[1]} Spalten\")\n",
    "print(f\"‚úì Target (y): {y.shape[0]} Werte\")\n",
    "print(f\"\\nVerbleibende Features:\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Schritt 3: Kategorische Variablen Encodieren\n",
    "\n",
    "Machine Learning Modelle k√∂nnen nur mit **Zahlen** arbeiten, nicht mit Text!\n",
    "\n",
    "**Beispiel:** Die Spalte 'Contract' hat Werte wie:\n",
    "- \"Month-to-Month\", \"One Year\", \"Two Year\"\n",
    "\n",
    "Nach **One-Hot Encoding** wird daraus:\n",
    "\n",
    "| Contract_One Year | Contract_Two Year |\n",
    "|-------------------|-------------------|\n",
    "| 0                 | 0                 | ‚Üê Month-to-Month\n",
    "| 1                 | 0                 | ‚Üê One Year\n",
    "| 0                 | 1                 | ‚Üê Two Year\n",
    "\n",
    "`drop_first=True`: Wir lassen eine Kategorie weg (Referenzkategorie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 3: KATEGORISCHE VARIABLEN ENCODIEREN\n",
    "# ============================================================================\n",
    "# One-Hot Encoding wandelt Text-Kategorien in Zahlen um.\n",
    "# \n",
    "# F√ºr jede Kategorie wird eine neue Spalte erstellt:\n",
    "#   - 1 = diese Kategorie trifft zu\n",
    "#   - 0 = diese Kategorie trifft nicht zu\n",
    "#\n",
    "# drop_first=True: Eine Kategorie wird weggelassen (Referenz)\n",
    "# Beispiel: Wenn Contract_One Year=0 und Contract_Two Year=0, \n",
    "#           dann MUSS es Month-to-Month sein.\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SCHRITT 3: KATEGORISCHE VARIABLEN ENCODIEREN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Kategorische Spalten finden (Datentyp = object)\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nKategorische Spalten ({len(cat_cols)}):\")\n",
    "for col in cat_cols:\n",
    "    unique_vals = X[col].nunique()\n",
    "    print(f\"   - {col}: {unique_vals} Kategorien\")\n",
    "    if unique_vals <= 10:  # Nur anzeigen wenn wenige Kategorien\n",
    "        print(f\"     Werte: {X[col].unique().tolist()}\")\n",
    "\n",
    "# One-Hot Encoding durchf√ºhren\n",
    "X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "print(f\"\\n‚úì Vorher:  {X.shape[1]} Spalten\")\n",
    "print(f\"‚úì Nachher: {X_encoded.shape[1]} Spalten (durch One-Hot Encoding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Schritt 4: Fehlende Werte Behandeln\n",
    "\n",
    "Fehlende Werte (NaN) k√∂nnen Modelle nicht verarbeiten!\n",
    "\n",
    "**Optionen:**\n",
    "1. Zeilen mit NaN l√∂schen (schlecht wenn viele Daten verloren gehen)\n",
    "2. NaN mit sinnvollen Werten f√ºllen (besser!)\n",
    "\n",
    "Wir verwenden den **Median** (Mittelwert der sortierten Daten):\n",
    "- Robust gegen Ausrei√üer\n",
    "- Beispiel: [1, 2, 3, 100] ‚Üí Median=2.5, Mean=26.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 4: FEHLENDE WERTE BEHANDELN\n",
    "# ============================================================================\n",
    "# Wir pr√ºfen auf fehlende Werte (NaN) und f√ºllen sie mit dem Median.\n",
    "# Der Median ist robust gegen Ausrei√üer.\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SCHRITT 4: FEHLENDE WERTE BEHANDELN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fehlende Werte z√§hlen\n",
    "missing = X_encoded.isnull().sum()\n",
    "missing_cols = missing[missing > 0]\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Spalten mit fehlenden Werten:\")\n",
    "    for col, count in missing_cols.items():\n",
    "        pct = count / len(X_encoded) * 100\n",
    "        print(f\"   - {col}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Mit Median auff√ºllen\n",
    "    X_encoded = X_encoded.fillna(X_encoded.median())\n",
    "    print(f\"\\n‚úì Fehlende Werte mit Median aufgef√ºllt!\")\n",
    "else:\n",
    "    print(\"\\n‚úì Keine fehlenden Werte gefunden!\")\n",
    "\n",
    "print(f\"\\nFinale Anzahl fehlender Werte: {X_encoded.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Schritt 5: Train/Test Split\n",
    "\n",
    "Wir teilen die Daten in zwei Teile:\n",
    "- **Training Set (80%)**: Damit LERNT das Modell\n",
    "- **Test Set (20%)**: Damit PR√úFEN wir wie gut das Modell ist\n",
    "\n",
    "**WICHTIG:** Das Modell sieht die Testdaten NIEMALS w√§hrend des Trainings!\n",
    "\n",
    "So simulieren wir \"echte\" neue Kunden, die das Modell noch nie gesehen hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 5: TRAIN/TEST SPLIT\n",
    "# ============================================================================\n",
    "# Wir teilen die Daten auf:\n",
    "#   - 80% zum Trainieren (das Modell lernt daraus)\n",
    "#   - 20% zum Testen (um zu pr√ºfen wie gut das Modell ist)\n",
    "#\n",
    "# stratify=y: Stellt sicher, dass Train und Test die gleiche \n",
    "#             Churn-Verteilung haben (wichtig bei unbalancierten Daten!)\n",
    "#\n",
    "# random_state=42: Macht das Ergebnis reproduzierbar\n",
    "#                  (gleiche Zufallszahlen bei jedem Durchlauf)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SCHRITT 5: TRAIN/TEST SPLIT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, \n",
    "    y, \n",
    "    test_size=0.2,       # 20% f√ºr Test\n",
    "    random_state=42,     # Reproduzierbarkeit\n",
    "    stratify=y           # Gleiche Churn-Verteilung\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Aufteilung:\")\n",
    "print(f\"   Training Set: {X_train.shape[0]} Samples ({X_train.shape[0]/len(X_encoded)*100:.0f}%)\")\n",
    "print(f\"   Test Set:     {X_test.shape[0]} Samples ({X_test.shape[0]/len(X_encoded)*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Churn-Verteilung im Training Set:\")\n",
    "print(f\"   No Churn (0): {(y_train == 0).sum()} ({(y_train == 0).mean()*100:.1f}%)\")\n",
    "print(f\"   Churn (1):    {(y_train == 1).sum()} ({(y_train == 1).mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Churn-Verteilung im Test Set:\")\n",
    "print(f\"   No Churn (0): {(y_test == 0).sum()} ({(y_test == 0).mean()*100:.1f}%)\")\n",
    "print(f\"   Churn (1):    {(y_test == 1).sum()} ({(y_test == 1).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Schritt 6: Features Skalieren\n",
    "\n",
    "Verschiedene Features haben verschiedene Skalen:\n",
    "- **Tenure in Months**: 0-72\n",
    "- **Total Revenue**: 0-10000+\n",
    "- **Monthly Charge**: 20-100\n",
    "\n",
    "**Problem:** Features mit gro√üen Werten dominieren das Modell!\n",
    "\n",
    "**StandardScaler** transformiert alle Features auf:\n",
    "- Mittelwert = 0\n",
    "- Standardabweichung = 1\n",
    "\n",
    "**WICHTIG:** \n",
    "- `fit_transform` auf TRAINING (lernt die Skalierung)\n",
    "- `transform` auf TEST (wendet gleiche Skalierung an)\n",
    "- NIE fit auf Testdaten! (sonst \"Data Leakage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 6: FEATURES SKALIEREN\n",
    "# ============================================================================\n",
    "# StandardScaler bringt alle Features auf die gleiche Skala:\n",
    "#   - Mittelwert = 0\n",
    "#   - Standardabweichung = 1\n",
    "#\n",
    "# Das ist wichtig f√ºr Modelle wie Logistic Regression und SVM!\n",
    "# Random Forest braucht keine Skalierung, schadet aber auch nicht.\n",
    "#\n",
    "# WICHTIG: Wir fitten NUR auf Trainingsdaten, dann transformieren wir beide!\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SCHRITT 6: FEATURES SKALIEREN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit_transform auf Training (lernt Mittelwert und Std)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# NUR transform auf Test (wendet gelernte Werte an)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Beispiel: Erste numerische Spalte\n",
    "example_col = X_train.columns[0]\n",
    "print(f\"\\nBeispiel Skalierung f√ºr '{example_col}':\")\n",
    "print(f\"   Vorher:  Min={X_train[example_col].min():.2f}, Max={X_train[example_col].max():.2f}\")\n",
    "print(f\"   Nachher: Min={X_train_scaled[:, 0].min():.2f}, Max={X_train_scaled[:, 0].max():.2f}\")\n",
    "\n",
    "print(\"\\n‚úì Features skaliert mit StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Schritt 7: Modelle Trainieren\n",
    "\n",
    "Wir trainieren verschiedene Algorithmen und vergleichen sie:\n",
    "\n",
    "| Modell | Beschreibung | Vorteile |\n",
    "|--------|--------------|----------|\n",
    "| **Logistic Regression** | Klassiker f√ºr bin√§re Klassifikation | Schnell, interpretierbar |\n",
    "| **Random Forest** | Ensemble aus vielen Entscheidungsb√§umen | Robust, keine Skalierung n√∂tig |\n",
    "| **Gradient Boosting** | B√§ume werden nacheinander trainiert | Oft beste Performance |\n",
    "| **K-Nearest Neighbors** | Klassifiziert nach n√§chsten Nachbarn | Einfach zu verstehen |\n",
    "| **SVM** | Findet optimale Trennlinie | Gut bei kleinen Datens√§tzen |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 7: MODELLE TRAINIEREN\n",
    "# ============================================================================\n",
    "# Wir trainieren 5 verschiedene Modelle und vergleichen ihre Performance.\n",
    "# \n",
    "# F√ºr jedes Modell berechnen wir:\n",
    "#   - Accuracy: Anteil korrekter Vorhersagen\n",
    "#   - ROC-AUC: Fl√§che unter der ROC-Kurve (0.5=Zufall, 1.0=Perfekt)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SCHRITT 7: MODELLE TRAINIEREN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Dictionary mit allen Modellen\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Ergebnisse speichern\n",
    "results = []\n",
    "\n",
    "print(\"\\nüöÄ Training l√§uft...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"   Training: {name}...\", end=\" \")\n",
    "    \n",
    "    # Modell trainieren (skalierte Daten f√ºr die meisten Modelle)\n",
    "    if name == 'Random Forest':\n",
    "        # Random Forest braucht keine Skalierung\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # Alle anderen Modelle mit skalierten Daten\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Metriken berechnen\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'ROC-AUC': auc,\n",
    "        'model_object': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_prob': y_prob\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úì Accuracy: {acc:.3f}, ROC-AUC: {auc:.3f}\")\n",
    "\n",
    "# Ergebnisse als DataFrame\n",
    "results_df = pd.DataFrame(results)[['Model', 'Accuracy', 'ROC-AUC']]\n",
    "results_df = results_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"üìä MODELL-VERGLEICH (sortiert nach ROC-AUC):\")\n",
    "print(\"-\" * 50)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Schritt 8: Evaluation - Confusion Matrix & ROC Curve\n",
    "\n",
    "### Confusion Matrix\n",
    "Zeigt die 4 m√∂glichen Outcomes:\n",
    "- **True Negative (TN)**: Kein Churn vorhergesagt, tats√§chlich kein Churn ‚úì\n",
    "- **True Positive (TP)**: Churn vorhergesagt, tats√§chlich Churn ‚úì\n",
    "- **False Negative (FN)**: Kein Churn vorhergesagt, aber tats√§chlich Churn ‚úó (SCHLECHT!)\n",
    "- **False Positive (FP)**: Churn vorhergesagt, aber kein Churn ‚úó\n",
    "\n",
    "### ROC Curve\n",
    "Zeigt den Trade-off zwischen:\n",
    "- **True Positive Rate**: Wie viele Churner erkennen wir?\n",
    "- **False Positive Rate**: Wie viele Nicht-Churner klassifizieren wir falsch?\n",
    "\n",
    "**AUC (Area Under Curve):**\n",
    "- 0.5 = Zuf√§lliges Raten\n",
    "- 0.7-0.8 = Akzeptabel\n",
    "- 0.8-0.9 = Gut\n",
    "- > 0.9 = Exzellent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 8: EVALUATION - CONFUSION MATRIX & ROC CURVE\n",
    "# ============================================================================\n",
    "# Wir visualisieren die Performance des besten Modells.\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SCHRITT 8: EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Bestes Modell finden\n",
    "best_result = max(results, key=lambda x: x['ROC-AUC'])\n",
    "best_model_name = best_result['Model']\n",
    "best_y_pred = best_result['y_pred']\n",
    "best_y_prob = best_result['y_prob']\n",
    "\n",
    "print(f\"\\nüèÜ Bestes Modell: {best_model_name}\")\n",
    "\n",
    "# Visualisierung\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No Churn', 'Churn'], \n",
    "            yticklabels=['No Churn', 'Churn'],\n",
    "            annot_kws={'size': 14})\n",
    "axes[0].set_title(f'Confusion Matrix ({best_model_name})', fontsize=12)\n",
    "axes[0].set_xlabel('Vorhergesagt', fontsize=11)\n",
    "axes[0].set_ylabel('Tats√§chlich', fontsize=11)\n",
    "\n",
    "# 2. ROC Curve f√ºr alle Modelle\n",
    "for result in results:\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['y_prob'])\n",
    "    auc = roc_auc_score(y_test, result['y_prob'])\n",
    "    axes[1].plot(fpr, tpr, label=f\"{result['Model']} (AUC={auc:.3f})\", linewidth=2)\n",
    "\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Zuf√§lliges Raten (AUC=0.5)', linewidth=1)\n",
    "axes[1].set_xlabel('False Positive Rate', fontsize=11)\n",
    "axes[1].set_ylabel('True Positive Rate', fontsize=11)\n",
    "axes[1].set_title('ROC Curve - Modellvergleich', fontsize=12)\n",
    "axes[1].legend(loc='lower right', fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix interpretieren\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nüìã Confusion Matrix Interpretation:\")\n",
    "print(f\"   ‚úì {tn} Kunden korrekt als 'No Churn' erkannt (True Negative)\")\n",
    "print(f\"   ‚úì {tp} Kunden korrekt als 'Churn' erkannt (True Positive)\")\n",
    "print(f\"   ‚úó {fp} Kunden f√§lschlich als 'Churn' markiert (False Positive)\")\n",
    "print(f\"   ‚úó {fn} Churner NICHT erkannt - das ist besonders schlecht! (False Negative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Schritt 9: Feature Importance\n",
    "\n",
    "Random Forest kann uns sagen, **welche Features am wichtigsten** sind!\n",
    "\n",
    "**Wie funktioniert das?**\n",
    "- Der Algorithmus misst, wie stark jedes Feature zur Trennung von Churn/No-Churn beitr√§gt\n",
    "- Features die oft f√ºr Splits verwendet werden = wichtiger\n",
    "\n",
    "**Das hilft uns zu verstehen:**\n",
    "- WARUM Kunden churnen\n",
    "- Welche Faktoren wir beeinflussen sollten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 9: FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "# Random Forest zeigt uns welche Features am wichtigsten f√ºr die\n",
    "# Churn-Vorhersage sind. Das gibt wertvolle Business Insights!\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SCHRITT 9: FEATURE IMPORTANCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Random Forest Modell aus den Ergebnissen holen\n",
    "rf_result = [r for r in results if r['Model'] == 'Random Forest'][0]\n",
    "rf_model = rf_result['model_object']\n",
    "\n",
    "# Feature Importance extrahieren\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_encoded.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Top 15 visualisieren\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_15 = feature_importance.head(15)\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(top_15)))\n",
    "sns.barplot(data=top_15, x='Importance', y='Feature', palette=colors)\n",
    "plt.title('Top 15 Wichtigste Features f√ºr Churn-Vorhersage', fontsize=14)\n",
    "plt.xlabel('Importance Score', fontsize=11)\n",
    "plt.ylabel('Feature', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Top 10 wichtigste Features:\")\n",
    "print(\"-\" * 40)\n",
    "for i, row in feature_importance.head(10).iterrows():\n",
    "    print(f\"   {row['Feature']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Schritt 10: Classification Report\n",
    "\n",
    "Detaillierte Metriken f√ºr jede Klasse:\n",
    "\n",
    "| Metrik | Beschreibung | Formel |\n",
    "|--------|--------------|--------|\n",
    "| **Precision** | Wenn das Modell 'Churn' sagt, wie oft stimmt das? | TP / (TP + FP) |\n",
    "| **Recall** | Von allen echten Churnern, wie viele erkennen wir? | TP / (TP + FN) |\n",
    "| **F1-Score** | Harmonic Mean von Precision und Recall | 2 √ó (P√óR)/(P+R) |\n",
    "\n",
    "**F√ºr Churn ist RECALL besonders wichtig!** Wir wollen m√∂glichst alle Churner finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 10: CLASSIFICATION REPORT\n",
    "# ============================================================================\n",
    "# Detaillierte Metriken f√ºr jede Klasse.\n",
    "#\n",
    "# PRECISION: \"Wenn das Modell 'Churn' sagt, wie oft stimmt das?\"\n",
    "# RECALL: \"Von allen echten Churnern, wie viele erkennen wir?\" (WICHTIG!)\n",
    "# F1-SCORE: Balance zwischen Precision und Recall\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SCHRITT 10: CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìä Detaillierter Report f√ºr {best_model_name}:\\n\")\n",
    "print(classification_report(y_test, best_y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "# Zusammenfassung\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ ZUSAMMENFASSUNG\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBestes Modell: {best_model_name}\")\n",
    "print(f\"Accuracy:      {accuracy_score(y_test, best_y_pred)*100:.1f}%\")\n",
    "print(f\"ROC-AUC:       {roc_auc_score(y_test, best_y_prob):.3f}\")\n",
    "print(f\"\\nDas Modell erkennt {tp} von {tp+fn} Churnern ({tp/(tp+fn)*100:.1f}% Recall)\")\n",
    "print(f\"Das Modell macht {fp} False Alarms (sagt Churn, aber kein Churn)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Modell-Vergleich Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BONUS: MODELL-VERGLEICH VISUALISIERUNG\n",
    "# ============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Daten vorbereiten\n",
    "models_sorted = results_df.sort_values('ROC-AUC', ascending=True)\n",
    "x = range(len(models_sorted))\n",
    "width = 0.35\n",
    "\n",
    "# Bars erstellen\n",
    "bars1 = ax.barh([i - width/2 for i in x], models_sorted['Accuracy'], width, \n",
    "                label='Accuracy', color='steelblue')\n",
    "bars2 = ax.barh([i + width/2 for i in x], models_sorted['ROC-AUC'], width, \n",
    "                label='ROC-AUC', color='coral')\n",
    "\n",
    "# Beschriftung\n",
    "ax.set_xlabel('Score', fontsize=11)\n",
    "ax.set_title('Modell-Vergleich: Accuracy vs ROC-AUC', fontsize=14)\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(models_sorted['Model'])\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "# Werte anzeigen\n",
    "for bar in bars1:\n",
    "    width_val = bar.get_width()\n",
    "    ax.text(width_val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "            f'{width_val:.3f}', va='center', fontsize=9)\n",
    "for bar in bars2:\n",
    "    width_val = bar.get_width()\n",
    "    ax.text(width_val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "            f'{width_val:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Modelltraining abgeschlossen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
