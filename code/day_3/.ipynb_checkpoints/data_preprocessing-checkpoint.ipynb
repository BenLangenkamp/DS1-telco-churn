{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "193b1f06",
   "metadata": {},
   "source": [
    "### Importing libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ab065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f77f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Source:\", \"https://www.kaggle.com/c/titanic/data\")\n",
    "\n",
    "# reading the data from a csv file\n",
    "data = pd.read_csv('../../data/day_3/titanic/titanic.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3589703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the metadata from a text file\n",
    "with open('../../data/day_3/titanic/titanic_meta.txt', 'r', encoding='utf-8') as f:\n",
    "    inhalt = f.read()\n",
    "    print(inhalt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9131d",
   "metadata": {},
   "source": [
    "### Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the \"survived\" column from the data\n",
    "input_data = data.drop(columns=[\"survived\"])\n",
    "\n",
    "# define the target variable separately\n",
    "target_variable = data[\"survived\"]\n",
    "\n",
    "# perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, target_variable, test_size=0.2, random_state=42)\n",
    "\n",
    "# show the length of the train set\n",
    "init_length_X_train = len(X_train)\n",
    "print(\"Train set size:\", init_length_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a867b82",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8346ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types of the columns\n",
    "print(\"Spalte, Eingelesener datentyp\", \"\\n\")\n",
    "\n",
    "object_cols = []\n",
    "for col in X_train.columns:\n",
    "    print(f\"{col}, {X_train[col].dtype}\")\n",
    "    if X_train[col].dtype == \"object\":\n",
    "        object_cols.append(col)\n",
    "        \n",
    "print(\"\\n\", \"MÃ¶glicherweise fehlerhafte Spalten:\", object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c8b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the unique values in each column marked as object type\n",
    "for col in object_cols:\n",
    "    print(col, X_train[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e33d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of missing values in each column\n",
    "missing_values = X_train.isnull().sum()\n",
    "print(missing_values, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63094658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns with less than X% missing values\n",
    "\n",
    "# define the threshold and calculate the absolute threshold number\n",
    "drop_below_relative = 0.01\n",
    "drop_below_absolute = drop_below_relative * len(X_train)\n",
    "\n",
    "# find out which columns have a number missing values below the threshold\n",
    "drop_cols = missing_values[(missing_values > 0) & (missing_values < drop_below_absolute)].index.tolist()\n",
    "print(\"The rows with missing values of the following columns will be dropped:\", \"\\n\", drop_cols, \"\\n\\n\")\n",
    "\n",
    "# find out which columns have a number missing values above the threshold\n",
    "not_drop_cols = missing_values[(missing_values > 0) & (missing_values > drop_below_absolute)].index.tolist()\n",
    "print(\"The rows with missing values of the following columns will be NOT dropped:\", \"\\n\", not_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6eb910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the rows to drop in a set to prevent duplicates\n",
    "rows_to_drop = set()\n",
    "\n",
    "# get the index of rows with missing values in these columns\n",
    "for col in drop_cols:\n",
    "    nan_rows = X_train[X_train[col].isnull()].index\n",
    "    rows_to_drop.update(nan_rows)\n",
    "\n",
    "# convert the set to a list\n",
    "rows_to_drop = list(rows_to_drop)\n",
    "\n",
    "# drop the rows from the dataset\n",
    "X_train.drop(rows_to_drop, inplace=True)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# drop the rows from the target variable\n",
    "y_train.drop(rows_to_drop, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Dropped\", init_length_X_train-len(X_train), \"rows with missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ab162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the remaining missing values\n",
    "\n",
    "# imputation by random sampling from the correct data of the column with missing data\n",
    "for col in not_drop_cols:\n",
    "    missing_mask = X_train[col].isnull()\n",
    "    num_imputed_values = missing_mask.sum()\n",
    "\n",
    "    sampled_values = X_train.loc[~missing_mask, col].sample(\n",
    "        n=num_imputed_values, \n",
    "        replace=True, \n",
    "        random_state=42\n",
    "    ).values\n",
    "\n",
    "    X_train.loc[missing_mask, col] = sampled_values\n",
    "    \n",
    "    print(f\"Imputed {num_imputed_values} values in column '{col}'\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundance\n",
    "\n",
    "# get the number of duplicate rows in the dataset\n",
    "num_duplicate_rows = X_train.duplicated().sum()\n",
    "print(f\"Number of identical rows in the dataset: {num_duplicate_rows}\", \"\\n\")\n",
    "\n",
    "print(\"Are those correct or incorrect duplicates?\", \"\\n\")\n",
    "\n",
    "# remove redundant columns\n",
    "X_train.drop(columns=[\"embark_town\"], inplace=True)\n",
    "print(\"Dropped 'embark_town' column as it is redundant to 'embarked' column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8dfbb",
   "metadata": {},
   "source": [
    "### Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc456f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal encoding - with order\n",
    "\n",
    "# define columns and mapping for the ordinal encoding\n",
    "ordinal_cols = ['class']\n",
    "ordinal_map = [['Third', 'Second', 'First']]\n",
    "\n",
    "# initialize the OrdinalEncoder with the specified categories\n",
    "ordinal_encoder = OrdinalEncoder(categories=ordinal_map)\n",
    "X_train[ordinal_cols] = ordinal_encoder.fit_transform(X_train[ordinal_cols])\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35bc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nominal encoding - without order (One-Hot Encoding)\n",
    "\n",
    "# define columns and for the nominal encoding\n",
    "nominal_cols = ['sex', 'embarked', 'who', 'deck']\n",
    "\n",
    "# use pandas inbuilt functions to one-hot encode the nominal columns\n",
    "X_train = pd.get_dummies(X_train, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2aff7",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0dcadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data set to float64\n",
    "X_train = X_train.astype('float64')\n",
    "\n",
    "# initialise the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# list of columns to be scaled\n",
    "columns_to_scale = ['age', 'fare']\n",
    "\n",
    "# fit & transform the columns\n",
    "X_train[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d82ff",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the familiy size could be an indicator of survival\n",
    "X_train[\"family_size\"] = X_train[\"sibsp\"] + X_train[\"parch\"] + 1\n",
    "\n",
    "# a combination of \"age\" and \"pclass\" could be an indicator of status and therefore survival\n",
    "X_train[\"age_class_interaction\"] = X_train[\"age\"] * X_train[\"pclass\"]\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d356b0",
   "metadata": {},
   "source": [
    "### Processing the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e97a8c",
   "metadata": {},
   "source": [
    "We need to process the test data in a similar way we processeds the training data.\n",
    "\n",
    "Otherwise the model will miss some of the artificial columns and values we created.\n",
    "\n",
    "We have to be very careful to not leak any information from the test set into our model, or from the training set into the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ef1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the redundant column\n",
    "X_test.drop(columns=['embark_town'], inplace=True)\n",
    "\n",
    "# impute missing values by random sampling ot the test data\n",
    "for col in X_test.columns:\n",
    "    if X_test[col].isnull:\n",
    "        missing_mask_test = X_test[col].isnull()\n",
    "        num_missing_test = missing_mask_test.sum()\n",
    "        if num_missing_test > 0:\n",
    "            sampled_values_test = X_test.loc[~missing_mask_test, col].sample(\n",
    "                n=num_missing_test,\n",
    "                replace=True,\n",
    "                random_state=42\n",
    "            ).values\n",
    "            X_test.loc[missing_mask_test, col] = sampled_values_test\n",
    "\n",
    "# ordinal encoding\n",
    "X_test[ordinal_cols] = ordinal_encoder.transform(X_test[ordinal_cols])\n",
    "\n",
    "# one-hot encoding\n",
    "X_test = pd.get_dummies(X_test, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "# add missing dummy columns (if they are not present in the test data)\n",
    "for col in X_train.columns:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = 0\n",
    "\n",
    "# convert the test data to float64 and reset the indices\n",
    "X_test = X_test.astype('float64')\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# scale the corresponding columns\n",
    "X_test[columns_to_scale] = scaler.transform(X_test[columns_to_scale]) # NO FIT-TRANSFORM HERE!\n",
    "\n",
    "# add the engineered features to the test data\n",
    "X_test[\"family_size\"]  = X_test[\"sibsp\"] + X_test[\"parch\"] + 1\n",
    "X_test[\"age_class_interaction\"]  = X_test[\"age\"] * X_test[\"pclass\"]\n",
    "\n",
    "X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
