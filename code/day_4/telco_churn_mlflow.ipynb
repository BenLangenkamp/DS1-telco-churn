{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telco Customer Churn - Model Training mit MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings unterdr√ºcken\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, roc_curve, roc_auc_score,\n",
    "                             precision_recall_curve, average_precision_score)\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Datenvorverarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4225, 52), Test: (1409, 52), Val: (1409, 52)\n"
     ]
    }
   ],
   "source": [
    "# daten laden\n",
    "train_data = pd.read_csv('../../data/day_3/telco-customer-churn/train.csv')\n",
    "test_data = pd.read_csv('../../data/day_3/telco-customer-churn/test.csv')\n",
    "val_data = pd.read_csv('../../data/day_3/telco-customer-churn/validation.csv')\n",
    "\n",
    "print(f\"Train: {train_data.shape}, Test: {test_data.shape}, Val: {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Churn Verteilung (Train):\n",
      "Churn\n",
      "0    3104\n",
      "1    1121\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Churn Rate: 26.5%\n"
     ]
    }
   ],
   "source": [
    "# churn verteilung checken\n",
    "print(\"Churn Verteilung (Train):\")\n",
    "print(train_data['Churn'].value_counts())\n",
    "print(f\"\\nChurn Rate: {train_data['Churn'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 40\n"
     ]
    }
   ],
   "source": [
    "# unn√∂tige spalten weg\n",
    "drop_cols = [\n",
    "    'Customer ID', 'Churn', 'Churn Category', 'Churn Reason', 'Customer Status',\n",
    "    'City', 'State', 'Country', 'Zip Code', 'Lat Long', 'Latitude', 'Longitude'\n",
    "]\n",
    "drop_cols = [c for c in drop_cols if c in train_data.columns]\n",
    "\n",
    "X_train = train_data.drop(columns=drop_cols)\n",
    "y_train = train_data['Churn']\n",
    "\n",
    "X_test = test_data.drop(columns=drop_cols)\n",
    "y_test = test_data['Churn']\n",
    "\n",
    "X_val = val_data.drop(columns=drop_cols)\n",
    "y_val = val_data['Churn']\n",
    "\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kategorische Spalten: 6\n",
      "Nach Encoding: 45 Features\n"
     ]
    }
   ],
   "source": [
    "# encoding\n",
    "cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Kategorische Spalten: {len(cat_cols)}\")\n",
    "\n",
    "X_train_enc = pd.get_dummies(X_train, columns=cat_cols, drop_first=True)\n",
    "X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)\n",
    "X_val_enc = pd.get_dummies(X_val, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# spalten angleichen\n",
    "for col in X_train_enc.columns:\n",
    "    if col not in X_test_enc.columns:\n",
    "        X_test_enc[col] = 0\n",
    "    if col not in X_val_enc.columns:\n",
    "        X_val_enc[col] = 0\n",
    "\n",
    "X_test_enc = X_test_enc[X_train_enc.columns]\n",
    "X_val_enc = X_val_enc[X_train_enc.columns]\n",
    "\n",
    "# missing values\n",
    "X_train_enc = X_train_enc.fillna(X_train_enc.median())\n",
    "X_test_enc = X_test_enc.fillna(X_train_enc.median())\n",
    "X_val_enc = X_val_enc.fillna(X_train_enc.median())\n",
    "\n",
    "print(f\"Nach Encoding: {X_train_enc.shape[1]} Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skalieren\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_enc)\n",
    "X_test_scaled = scaler.transform(X_test_enc)\n",
    "X_val_scaled = scaler.transform(X_val_enc)\n",
    "\n",
    "# als DataFrame f√ºr MLflow\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train_enc.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_train_enc.columns)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_train_enc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y als flat arrays\n",
    "y_train_flat = y_train.values.ravel()\n",
    "y_test_flat = y_test.values.ravel()\n",
    "y_val_flat = y_val.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Model Training mit MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/29 13:36:38 INFO mlflow.tracking.fluent: Experiment with name 'Telco Churn Classification' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/1', creation_time=1769693798493, experiment_id='1', last_update_time=1769693798493, lifecycle_stage='active', name='Telco Churn Classification', tags={}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlflow setup\n",
    "mlflow.set_tracking_uri(\"http://mlflow_server:5000\")\n",
    "mlflow.set_experiment(\"Telco Churn Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definieren der Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base models\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbc = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='distance'\n",
    ")\n",
    "\n",
    "svc = SVC(\n",
    "    C=1.0,\n",
    "    kernel='rbf',\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch models\n",
    "grid_log_reg = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    param_grid={\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    cv=5, n_jobs=-1, verbose=1, scoring='recall'\n",
    ")\n",
    "\n",
    "grid_rfc = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid={\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    cv=5, n_jobs=-1, verbose=1, scoring='recall'\n",
    ")\n",
    "\n",
    "grid_gbc = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_grid={\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.05, 0.1, 0.2]\n",
    "    },\n",
    "    cv=5, n_jobs=-1, verbose=1, scoring='recall'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(model_name, y_true, y_prob):\n",
    "    \"\"\"ROC Curve erstellen und speichern\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plot_path = f\"roc_curve_{model_name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model_name, y_true, y_pred):\n",
    "    \"\"\"Confusion Matrix erstellen und speichern\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['No Churn', 'Churn'],\n",
    "                yticklabels=['No Churn', 'Churn'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    \n",
    "    plot_path = f\"confusion_matrix_{model_name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(model_name, y_true, y_prob):\n",
    "    \"\"\"Precision-Recall Curve erstellen und speichern\"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR (AP = {ap:.3f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve - {model_name}')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plot_path = f\"pr_curve_{model_name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Training Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_log_model(\n",
    "    model_obj,\n",
    "    model_name,\n",
    "    X_train_data,\n",
    "    y_train_data,\n",
    "    X_test_data,\n",
    "    y_test_data,\n",
    "    feature_columns,\n",
    "    registered_model_name,\n",
    "    needs_scaling=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Trainiert, evaluiert und loggt ein Model mit MLflow.\n",
    "    Fokus auf RECALL da wir Churner erkennen wollen!\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        print(f\"\\n--- Starting MLflow run for: {model_name} ---\")\n",
    "        \n",
    "        is_grid_search = isinstance(model_obj, GridSearchCV)\n",
    "        \n",
    "        # training\n",
    "        print(f\"  Training {model_name}...\")\n",
    "        start_time = time()\n",
    "        model_obj.fit(X_train_data, y_train_data)\n",
    "        duration = time() - start_time\n",
    "        \n",
    "        if is_grid_search:\n",
    "            print(f\"  Tuning time: {duration:.2f}s\")\n",
    "            print(f\"  Best params: {model_obj.best_params_}\")\n",
    "            mlflow.log_metric(\"tuning_time_seconds\", duration)\n",
    "            mlflow.log_params(model_obj.best_params_)\n",
    "            trained_model = model_obj.best_estimator_\n",
    "        else:\n",
    "            print(f\"  Training time: {duration:.2f}s\")\n",
    "            mlflow.log_metric(\"training_time_seconds\", duration)\n",
    "            mlflow.log_params(model_obj.get_params())\n",
    "            trained_model = model_obj\n",
    "        \n",
    "        # feature importance loggen (wenn vorhanden)\n",
    "        if hasattr(trained_model, 'feature_importances_') and feature_columns is not None:\n",
    "            feat_imp = {str(feature_columns[i]): float(trained_model.feature_importances_[i]) \n",
    "                       for i in range(len(feature_columns))}\n",
    "            mlflow.log_dict(feat_imp, \"feature_importances.json\")\n",
    "            print(\"  Feature importances logged.\")\n",
    "        \n",
    "        # model loggen und registrieren\n",
    "        mlflow.sklearn.log_model(\n",
    "            trained_model,\n",
    "            \"model\",\n",
    "            input_example=X_train_data.head(1) if hasattr(X_train_data, 'head') else X_train_data[:1],\n",
    "            registered_model_name=registered_model_name\n",
    "        )\n",
    "        print(f\"  Model registered as '{registered_model_name}'\")\n",
    "        \n",
    "        # evaluation\n",
    "        print(f\"  Evaluating {model_name}...\")\n",
    "        y_pred = trained_model.predict(X_test_data)\n",
    "        y_prob = trained_model.predict_proba(X_test_data)[:, 1]\n",
    "        \n",
    "        # metriken berechnen\n",
    "        accuracy = accuracy_score(y_test_data, y_pred)\n",
    "        precision = precision_score(y_test_data, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test_data, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test_data, y_pred, zero_division=0)\n",
    "        auc = roc_auc_score(y_test_data, y_prob)\n",
    "        \n",
    "        # weighted score (2x recall + 1x precision) / 3\n",
    "        weighted_score = (2 * recall + precision) / 3\n",
    "        \n",
    "        # metriken loggen\n",
    "        mlflow.log_metric(\"test_accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"test_precision\", precision)\n",
    "        mlflow.log_metric(\"test_recall\", recall)\n",
    "        mlflow.log_metric(\"test_f1_score\", f1)\n",
    "        mlflow.log_metric(\"test_roc_auc\", auc)\n",
    "        mlflow.log_metric(\"test_weighted_score\", weighted_score)\n",
    "        \n",
    "        print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f} <-- wichtigste Metrik!\")\n",
    "        print(f\"  F1-Score:  {f1:.4f}\")\n",
    "        print(f\"  ROC-AUC:   {auc:.4f}\")\n",
    "        print(f\"  Weighted:  {weighted_score:.4f}\")\n",
    "        \n",
    "        # plots erstellen und loggen\n",
    "        cm_path = plot_confusion_matrix(model_name, y_test_data, y_pred)\n",
    "        mlflow.log_artifact(cm_path)\n",
    "        os.remove(cm_path)\n",
    "        \n",
    "        roc_path = plot_roc_curve(model_name, y_test_data, y_prob)\n",
    "        mlflow.log_artifact(roc_path)\n",
    "        os.remove(roc_path)\n",
    "        \n",
    "        pr_path = plot_precision_recall_curve(model_name, y_test_data, y_prob)\n",
    "        mlflow.log_artifact(pr_path)\n",
    "        os.remove(pr_path)\n",
    "        \n",
    "        print(\"  Plots logged.\")\n",
    "        print(f\"--- Finished MLflow run for: {model_name} ---\")\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'recall': recall,\n",
    "            'weighted_score': weighted_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Runs ausf√ºhren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# registered model names\n",
    "registered_model_names = {\n",
    "    \"LogisticRegression\": \"ChurnClassifier_LogisticRegression\",\n",
    "    \"RandomForest\": \"ChurnClassifier_RandomForest\",\n",
    "    \"GradientBoosting\": \"ChurnClassifier_GradientBoosting\",\n",
    "    \"KNN\": \"ChurnClassifier_KNN\",\n",
    "    \"SVC\": \"ChurnClassifier_SVC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alle runs speichern f√ºr vergleich\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting MLflow run for: Base LogisticRegression ---\n",
      "  Training Base LogisticRegression...\n",
      "  Training time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'ChurnClassifier_LogisticRegression'.\n",
      "2026/01/29 13:36:44 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ChurnClassifier_LogisticRegression, version 1\n",
      "Created version '1' of model 'ChurnClassifier_LogisticRegression'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model registered as 'ChurnClassifier_LogisticRegression'\n",
      "  Evaluating Base LogisticRegression...\n",
      "  Accuracy:  0.9780\n",
      "  Precision: 0.9648\n",
      "  Recall:    0.9519 <-- wichtigste Metrik!\n",
      "  F1-Score:  0.9583\n",
      "  ROC-AUC:   0.9974\n",
      "  Weighted:  0.9562\n",
      "  Plots logged.\n",
      "--- Finished MLflow run for: Base LogisticRegression ---\n",
      "üèÉ View run Base LogisticRegression at: http://mlflow_server:5000/#/experiments/1/runs/dac2e05196b14a05b3a9eaa35d74c8af\n",
      "üß™ View experiment at: http://mlflow_server:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# base logistic regression\n",
    "result = train_evaluate_log_model(\n",
    "    log_reg, \"Base LogisticRegression\",\n",
    "    X_train_scaled_df, y_train_flat,\n",
    "    X_test_scaled_df, y_test_flat,\n",
    "    X_train_enc.columns.tolist(),\n",
    "    registered_model_names[\"LogisticRegression\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting MLflow run for: Base RandomForest ---\n",
      "  Training Base RandomForest...\n",
      "  Training time: 0.34s\n",
      "  Feature importances logged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'ChurnClassifier_RandomForest'.\n",
      "2026/01/29 13:36:48 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ChurnClassifier_RandomForest, version 1\n",
      "Created version '1' of model 'ChurnClassifier_RandomForest'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model registered as 'ChurnClassifier_RandomForest'\n",
      "  Evaluating Base RandomForest...\n",
      "  Accuracy:  0.9773\n",
      "  Precision: 0.9886\n",
      "  Recall:    0.9251 <-- wichtigste Metrik!\n",
      "  F1-Score:  0.9558\n",
      "  ROC-AUC:   0.9966\n",
      "  Weighted:  0.9463\n",
      "  Plots logged.\n",
      "--- Finished MLflow run for: Base RandomForest ---\n",
      "üèÉ View run Base RandomForest at: http://mlflow_server:5000/#/experiments/1/runs/ba4edf26623c41c495d19d020f155731\n",
      "üß™ View experiment at: http://mlflow_server:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# base random forest (braucht keine skalierung)\n",
    "result = train_evaluate_log_model(\n",
    "    rfc, \"Base RandomForest\",\n",
    "    X_train_enc, y_train_flat,\n",
    "    X_test_enc, y_test_flat,\n",
    "    X_train_enc.columns.tolist(),\n",
    "    registered_model_names[\"RandomForest\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting MLflow run for: Base GradientBoosting ---\n",
      "  Training Base GradientBoosting...\n",
      "  Training time: 1.86s\n",
      "  Feature importances logged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'ChurnClassifier_GradientBoosting'.\n",
      "2026/01/29 13:36:54 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ChurnClassifier_GradientBoosting, version 1\n",
      "Created version '1' of model 'ChurnClassifier_GradientBoosting'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model registered as 'ChurnClassifier_GradientBoosting'\n",
      "  Evaluating Base GradientBoosting...\n",
      "  Accuracy:  0.9808\n",
      "  Precision: 0.9780\n",
      "  Recall:    0.9492 <-- wichtigste Metrik!\n",
      "  F1-Score:  0.9634\n",
      "  ROC-AUC:   0.9986\n",
      "  Weighted:  0.9588\n",
      "  Plots logged.\n",
      "--- Finished MLflow run for: Base GradientBoosting ---\n",
      "üèÉ View run Base GradientBoosting at: http://mlflow_server:5000/#/experiments/1/runs/26db7eab0bb24e1a82da0d100febe317\n",
      "üß™ View experiment at: http://mlflow_server:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# base gradient boosting\n",
    "result = train_evaluate_log_model(\n",
    "    gbc, \"Base GradientBoosting\",\n",
    "    X_train_enc, y_train_flat,\n",
    "    X_test_enc, y_test_flat,\n",
    "    X_train_enc.columns.tolist(),\n",
    "    registered_model_names[\"GradientBoosting\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting MLflow run for: Base KNN ---\n",
      "  Training Base KNN...\n",
      "  Training time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'ChurnClassifier_KNN'.\n",
      "2026/01/29 13:36:57 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ChurnClassifier_KNN, version 1\n",
      "Created version '1' of model 'ChurnClassifier_KNN'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model registered as 'ChurnClassifier_KNN'\n",
      "  Evaluating Base KNN...\n",
      "  Accuracy:  0.9099\n",
      "  Precision: 0.8241\n",
      "  Recall:    0.8396 <-- wichtigste Metrik!\n",
      "  F1-Score:  0.8318\n",
      "  ROC-AUC:   0.9647\n",
      "  Weighted:  0.8344\n",
      "  Plots logged.\n",
      "--- Finished MLflow run for: Base KNN ---\n",
      "üèÉ View run Base KNN at: http://mlflow_server:5000/#/experiments/1/runs/eea0063dda804533868e7df15228b00e\n",
      "üß™ View experiment at: http://mlflow_server:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# base knn\n",
    "result = train_evaluate_log_model(\n",
    "    knn, \"Base KNN\",\n",
    "    X_train_scaled_df, y_train_flat,\n",
    "    X_test_scaled_df, y_test_flat,\n",
    "    None,\n",
    "    registered_model_names[\"KNN\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting MLflow run for: Base SVC ---\n",
      "  Training Base SVC...\n",
      "  Training time: 0.45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'ChurnClassifier_SVC'.\n",
      "2026/01/29 13:37:01 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ChurnClassifier_SVC, version 1\n",
      "Created version '1' of model 'ChurnClassifier_SVC'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model registered as 'ChurnClassifier_SVC'\n",
      "  Evaluating Base SVC...\n",
      "  Accuracy:  0.9801\n",
      "  Precision: 0.9727\n",
      "  Recall:    0.9519 <-- wichtigste Metrik!\n",
      "  F1-Score:  0.9622\n",
      "  ROC-AUC:   0.9971\n",
      "  Weighted:  0.9588\n",
      "  Plots logged.\n",
      "--- Finished MLflow run for: Base SVC ---\n",
      "üèÉ View run Base SVC at: http://mlflow_server:5000/#/experiments/1/runs/26b734cb819e4416bc3e8b675f58ce7f\n",
      "üß™ View experiment at: http://mlflow_server:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# base svc\n",
    "result = train_evaluate_log_model(\n",
    "    svc, \"Base SVC\",\n",
    "    X_train_scaled_df, y_train_flat,\n",
    "    X_test_scaled_df, y_test_flat,\n",
    "    None,\n",
    "    registered_model_names[\"SVC\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting MLflow run for: Tuned LogisticRegression (GridSearch) ---\n",
      "  Training Tuned LogisticRegression (GridSearch)...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "  Tuning time: 2.80s\n",
      "  Best params: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'ChurnClassifier_LogisticRegression' already exists. Creating a new version of this model...\n",
      "2026/01/29 13:37:07 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ChurnClassifier_LogisticRegression, version 2\n",
      "Created version '2' of model 'ChurnClassifier_LogisticRegression'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model registered as 'ChurnClassifier_LogisticRegression'\n",
      "  Evaluating Tuned LogisticRegression (GridSearch)...\n",
      "  Accuracy:  0.9815\n",
      "  Precision: 0.9703\n",
      "  Recall:    0.9599 <-- wichtigste Metrik!\n",
      "  F1-Score:  0.9651\n",
      "  ROC-AUC:   0.9978\n",
      "  Weighted:  0.9634\n",
      "  Plots logged.\n",
      "--- Finished MLflow run for: Tuned LogisticRegression (GridSearch) ---\n",
      "üèÉ View run Tuned LogisticRegression (GridSearch) at: http://mlflow_server:5000/#/experiments/1/runs/f224d74a4f874abab2aa6ce44f855665\n",
      "üß™ View experiment at: http://mlflow_server:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# tuned logistic regression\n",
    "result = train_evaluate_log_model(\n",
    "    grid_log_reg, \"Tuned LogisticRegression (GridSearch)\",\n",
    "    X_train_scaled_df, y_train_flat,\n",
    "    X_test_scaled_df, y_test_flat,\n",
    "    X_train_enc.columns.tolist(),\n",
    "    registered_model_names[\"LogisticRegression\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting MLflow run for: Tuned RandomForest (GridSearch) ---\n",
      "  Training Tuned RandomForest (GridSearch)...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "  Tuning time: 6.75s\n",
      "  Best params: {'max_depth': 15, 'min_samples_leaf': 1, 'n_estimators': 50}\n",
      "  Feature importances logged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'ChurnClassifier_RandomForest' already exists. Creating a new version of this model...\n",
      "2026/01/29 13:37:18 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ChurnClassifier_RandomForest, version 2\n",
      "Created version '2' of model 'ChurnClassifier_RandomForest'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model registered as 'ChurnClassifier_RandomForest'\n",
      "  Evaluating Tuned RandomForest (GridSearch)...\n",
      "  Accuracy:  0.9780\n",
      "  Precision: 0.9858\n",
      "  Recall:    0.9305 <-- wichtigste Metrik!\n",
      "  F1-Score:  0.9574\n",
      "  ROC-AUC:   0.9974\n",
      "  Weighted:  0.9489\n",
      "  Plots logged.\n",
      "--- Finished MLflow run for: Tuned RandomForest (GridSearch) ---\n",
      "üèÉ View run Tuned RandomForest (GridSearch) at: http://mlflow_server:5000/#/experiments/1/runs/05b18a80a29844b593a31b1ab02749de\n",
      "üß™ View experiment at: http://mlflow_server:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# tuned random forest\n",
    "result = train_evaluate_log_model(\n",
    "    grid_rfc, \"Tuned RandomForest (GridSearch)\",\n",
    "    X_train_enc, y_train_flat,\n",
    "    X_test_enc, y_test_flat,\n",
    "    X_train_enc.columns.tolist(),\n",
    "    registered_model_names[\"RandomForest\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting MLflow run for: Tuned GradientBoosting (GridSearch) ---\n",
      "  Training Tuned GradientBoosting (GridSearch)...\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "  Tuning time: 18.13s\n",
      "  Best params: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 50}\n",
      "  Feature importances logged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'ChurnClassifier_GradientBoosting' already exists. Creating a new version of this model...\n",
      "2026/01/29 13:37:40 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ChurnClassifier_GradientBoosting, version 2\n",
      "Created version '2' of model 'ChurnClassifier_GradientBoosting'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model registered as 'ChurnClassifier_GradientBoosting'\n",
      "  Evaluating Tuned GradientBoosting (GridSearch)...\n",
      "  Accuracy:  0.9823\n",
      "  Precision: 0.9781\n",
      "  Recall:    0.9545 <-- wichtigste Metrik!\n",
      "  F1-Score:  0.9662\n",
      "  ROC-AUC:   0.9987\n",
      "  Weighted:  0.9624\n",
      "  Plots logged.\n",
      "--- Finished MLflow run for: Tuned GradientBoosting (GridSearch) ---\n",
      "üèÉ View run Tuned GradientBoosting (GridSearch) at: http://mlflow_server:5000/#/experiments/1/runs/0bdd45332cdf4420ae45aa105ea3420f\n",
      "üß™ View experiment at: http://mlflow_server:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# tuned gradient boosting\n",
    "result = train_evaluate_log_model(\n",
    "    grid_gbc, \"Tuned GradientBoosting (GridSearch)\",\n",
    "    X_train_enc, y_train_flat,\n",
    "    X_test_enc, y_test_flat,\n",
    "    X_train_enc.columns.tolist(),\n",
    "    registered_model_names[\"GradientBoosting\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "All MLflow runs complete!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All MLflow runs complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnisse vergleichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modell-Ranking (nach Weighted Score = 2x Recall + 1x Precision):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>recall</th>\n",
       "      <th>weighted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tuned LogisticRegression (GridSearch)</td>\n",
       "      <td>0.959893</td>\n",
       "      <td>0.963352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tuned GradientBoosting (GridSearch)</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.962391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Base SVC</td>\n",
       "      <td>0.951872</td>\n",
       "      <td>0.958807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Base GradientBoosting</td>\n",
       "      <td>0.949198</td>\n",
       "      <td>0.958786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base LogisticRegression</td>\n",
       "      <td>0.951872</td>\n",
       "      <td>0.956171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tuned RandomForest (GridSearch)</td>\n",
       "      <td>0.930481</td>\n",
       "      <td>0.948933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Base RandomForest</td>\n",
       "      <td>0.925134</td>\n",
       "      <td>0.946280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Base KNN</td>\n",
       "      <td>0.839572</td>\n",
       "      <td>0.834430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model_name    recall  weighted_score\n",
       "5  Tuned LogisticRegression (GridSearch)  0.959893        0.963352\n",
       "7    Tuned GradientBoosting (GridSearch)  0.954545        0.962391\n",
       "4                               Base SVC  0.951872        0.958807\n",
       "2                  Base GradientBoosting  0.949198        0.958786\n",
       "0                Base LogisticRegression  0.951872        0.956171\n",
       "6        Tuned RandomForest (GridSearch)  0.930481        0.948933\n",
       "1                      Base RandomForest  0.925134        0.946280\n",
       "3                               Base KNN  0.839572        0.834430"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ergebnisse als dataframe\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values('weighted_score', ascending=False)\n",
    "print(\"\\nModell-Ranking (nach Weighted Score = 2x Recall + 1x Precision):\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Champion Modell bestimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MLflow Champion Promotion ---\n",
      "Champion wird basierend auf 'test_weighted_score' ausgew√§hlt\n",
      "(Weighted Score = 2x Recall + 1x Precision, weil Recall wichtiger ist!)\n",
      "\n",
      "--- Collecting All Model Versions ---\n",
      "  Processing: ChurnClassifier_GradientBoosting\n",
      "    Version 2: test_weighted_score=0.9624\n",
      "    Version 1: test_weighted_score=0.9588\n",
      "  Processing: ChurnClassifier_KNN\n",
      "    Version 1: test_weighted_score=0.8344\n",
      "  Processing: ChurnClassifier_LogisticRegression\n",
      "    Version 2: test_weighted_score=0.9634\n",
      "    Version 1: test_weighted_score=0.9562\n",
      "  Processing: ChurnClassifier_RandomForest\n",
      "    Version 2: test_weighted_score=0.9489\n",
      "    Version 1: test_weighted_score=0.9463\n",
      "  Processing: ChurnClassifier_SVC\n",
      "    Version 1: test_weighted_score=0.9588\n",
      "\n",
      "--- Champion gefunden ---\n",
      "Model: ChurnClassifier_LogisticRegression\n",
      "Version: 2\n",
      "test_weighted_score: 0.9634\n",
      "\n",
      "‚úì Champion alias gesetzt auf ChurnClassifier_LogisticRegression Version 2\n",
      "\n",
      "--- Champion Promotion abgeschlossen ---\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "# wir nutzen weighted_score als champion metrik (2x recall + precision)\n",
    "CHAMPION_METRIC = \"test_weighted_score\"\n",
    "GLOBAL_CHAMPION_ALIAS = \"champion\"\n",
    "\n",
    "print(f\"--- MLflow Champion Promotion ---\")\n",
    "print(f\"Champion wird basierend auf '{CHAMPION_METRIC}' ausgew√§hlt\")\n",
    "print(f\"(Weighted Score = 2x Recall + 1x Precision, weil Recall wichtiger ist!)\")\n",
    "\n",
    "try:\n",
    "    registered_models = client.search_registered_models()\n",
    "    if not registered_models:\n",
    "        print(\"Keine registrierten Modelle gefunden.\")\n",
    "    else:\n",
    "        print(f\"\\n--- Collecting All Model Versions ---\")\n",
    "        all_versions = []\n",
    "        \n",
    "        for rm in registered_models:\n",
    "            if not rm.name.startswith(\"ChurnClassifier\"):\n",
    "                continue\n",
    "                \n",
    "            print(f\"  Processing: {rm.name}\")\n",
    "            model_versions = client.search_model_versions(f\"name='{rm.name}'\")\n",
    "            \n",
    "            for mv in model_versions:\n",
    "                try:\n",
    "                    run = client.get_run(mv.run_id)\n",
    "                    metric_value = run.data.metrics.get(CHAMPION_METRIC)\n",
    "                    \n",
    "                    if metric_value is not None:\n",
    "                        all_versions.append({\n",
    "                            \"registered_model_name\": rm.name,\n",
    "                            \"version\": mv.version,\n",
    "                            \"run_id\": mv.run_id,\n",
    "                            \"metric_value\": metric_value,\n",
    "                            \"aliases\": mv.aliases\n",
    "                        })\n",
    "                        print(f\"    Version {mv.version}: {CHAMPION_METRIC}={metric_value:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error: {e}\")\n",
    "        \n",
    "        if all_versions:\n",
    "            # bestes modell finden\n",
    "            best = max(all_versions, key=lambda x: x['metric_value'])\n",
    "            print(f\"\\n--- Champion gefunden ---\")\n",
    "            print(f\"Model: {best['registered_model_name']}\")\n",
    "            print(f\"Version: {best['version']}\")\n",
    "            print(f\"{CHAMPION_METRIC}: {best['metric_value']:.4f}\")\n",
    "            \n",
    "            # champion alias setzen\n",
    "            # erst alte aliases entfernen\n",
    "            for v in all_versions:\n",
    "                if GLOBAL_CHAMPION_ALIAS in v['aliases']:\n",
    "                    client.delete_registered_model_alias(\n",
    "                        name=v['registered_model_name'],\n",
    "                        alias=GLOBAL_CHAMPION_ALIAS\n",
    "                    )\n",
    "                    print(f\"  Removed '{GLOBAL_CHAMPION_ALIAS}' from {v['registered_model_name']} v{v['version']}\")\n",
    "            \n",
    "            # neuen champion setzen\n",
    "            client.set_registered_model_alias(\n",
    "                name=best['registered_model_name'],\n",
    "                alias=GLOBAL_CHAMPION_ALIAS,\n",
    "                version=best['version']\n",
    "            )\n",
    "            print(f\"\\n‚úì Champion alias gesetzt auf {best['registered_model_name']} Version {best['version']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Champion Promotion abgeschlossen ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fertig! Check MLflow UI f√ºr alle Ergebnisse.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFertig! Check MLflow UI f√ºr alle Ergebnisse.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
