{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telco Customer Churn - Model Training mit MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings unterdrücken\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, roc_curve, roc_auc_score,\n",
    "                             precision_recall_curve, average_precision_score)\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Datenvorverarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daten laden\n",
    "train_data = pd.read_csv('../../data/day_3/telco-customer-churn/train.csv')\n",
    "test_data = pd.read_csv('../../data/day_3/telco-customer-churn/test.csv')\n",
    "val_data = pd.read_csv('../../data/day_3/telco-customer-churn/validation.csv')\n",
    "\n",
    "print(f\"Train: {train_data.shape}, Test: {test_data.shape}, Val: {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# churn verteilung checken\n",
    "print(\"Churn Verteilung (Train):\")\n",
    "print(train_data['Churn'].value_counts())\n",
    "print(f\"\\nChurn Rate: {train_data['Churn'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnötige spalten weg\n",
    "drop_cols = [\n",
    "    'Customer ID', 'Churn', 'Churn Category', 'Churn Reason', 'Customer Status',\n",
    "    'City', 'State', 'Country', 'Zip Code', 'Lat Long', 'Latitude', 'Longitude'\n",
    "]\n",
    "drop_cols = [c for c in drop_cols if c in train_data.columns]\n",
    "\n",
    "X_train = train_data.drop(columns=drop_cols)\n",
    "y_train = train_data['Churn']\n",
    "\n",
    "X_test = test_data.drop(columns=drop_cols)\n",
    "y_test = test_data['Churn']\n",
    "\n",
    "X_val = val_data.drop(columns=drop_cols)\n",
    "y_val = val_data['Churn']\n",
    "\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding\n",
    "cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Kategorische Spalten: {len(cat_cols)}\")\n",
    "\n",
    "X_train_enc = pd.get_dummies(X_train, columns=cat_cols, drop_first=True)\n",
    "X_test_enc = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)\n",
    "X_val_enc = pd.get_dummies(X_val, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# spalten angleichen\n",
    "for col in X_train_enc.columns:\n",
    "    if col not in X_test_enc.columns:\n",
    "        X_test_enc[col] = 0\n",
    "    if col not in X_val_enc.columns:\n",
    "        X_val_enc[col] = 0\n",
    "\n",
    "X_test_enc = X_test_enc[X_train_enc.columns]\n",
    "X_val_enc = X_val_enc[X_train_enc.columns]\n",
    "\n",
    "# missing values\n",
    "X_train_enc = X_train_enc.fillna(X_train_enc.median())\n",
    "X_test_enc = X_test_enc.fillna(X_train_enc.median())\n",
    "X_val_enc = X_val_enc.fillna(X_train_enc.median())\n",
    "\n",
    "print(f\"Nach Encoding: {X_train_enc.shape[1]} Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skalieren\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_enc)\n",
    "X_test_scaled = scaler.transform(X_test_enc)\n",
    "X_val_scaled = scaler.transform(X_val_enc)\n",
    "\n",
    "# als DataFrame für MLflow\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train_enc.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_train_enc.columns)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_train_enc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y als flat arrays\n",
    "y_train_flat = y_train.values.ravel()\n",
    "y_test_flat = y_test.values.ravel()\n",
    "y_val_flat = y_val.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Model Training mit MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow setup\n",
    "mlflow.set_tracking_uri(\"http://mlflow_server:5000\")\n",
    "mlflow.set_experiment(\"Telco Churn Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definieren der Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base models\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbc = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='distance'\n",
    ")\n",
    "\n",
    "svc = SVC(\n",
    "    C=1.0,\n",
    "    kernel='rbf',\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch models\n",
    "grid_log_reg = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    param_grid={\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    cv=5, n_jobs=-1, verbose=1, scoring='recall'\n",
    ")\n",
    "\n",
    "grid_rfc = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid={\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    cv=5, n_jobs=-1, verbose=1, scoring='recall'\n",
    ")\n",
    "\n",
    "grid_gbc = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_grid={\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.05, 0.1, 0.2]\n",
    "    },\n",
    "    cv=5, n_jobs=-1, verbose=1, scoring='recall'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(model_name, y_true, y_prob):\n",
    "    \"\"\"ROC Curve erstellen und speichern\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plot_path = f\"roc_curve_{model_name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model_name, y_true, y_pred):\n",
    "    \"\"\"Confusion Matrix erstellen und speichern\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['No Churn', 'Churn'],\n",
    "                yticklabels=['No Churn', 'Churn'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    \n",
    "    plot_path = f\"confusion_matrix_{model_name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(model_name, y_true, y_prob):\n",
    "    \"\"\"Precision-Recall Curve erstellen und speichern\"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR (AP = {ap:.3f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve - {model_name}')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plot_path = f\"pr_curve_{model_name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Training Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_log_model(\n",
    "    model_obj,\n",
    "    model_name,\n",
    "    X_train_data,\n",
    "    y_train_data,\n",
    "    X_test_data,\n",
    "    y_test_data,\n",
    "    feature_columns,\n",
    "    registered_model_name,\n",
    "    needs_scaling=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Trainiert, evaluiert und loggt ein Model mit MLflow.\n",
    "    Fokus auf RECALL da wir Churner erkennen wollen!\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        print(f\"\\n--- Starting MLflow run for: {model_name} ---\")\n",
    "        \n",
    "        is_grid_search = isinstance(model_obj, GridSearchCV)\n",
    "        \n",
    "        # training\n",
    "        print(f\"  Training {model_name}...\")\n",
    "        start_time = time()\n",
    "        model_obj.fit(X_train_data, y_train_data)\n",
    "        duration = time() - start_time\n",
    "        \n",
    "        if is_grid_search:\n",
    "            print(f\"  Tuning time: {duration:.2f}s\")\n",
    "            print(f\"  Best params: {model_obj.best_params_}\")\n",
    "            mlflow.log_metric(\"tuning_time_seconds\", duration)\n",
    "            mlflow.log_params(model_obj.best_params_)\n",
    "            trained_model = model_obj.best_estimator_\n",
    "        else:\n",
    "            print(f\"  Training time: {duration:.2f}s\")\n",
    "            mlflow.log_metric(\"training_time_seconds\", duration)\n",
    "            mlflow.log_params(model_obj.get_params())\n",
    "            trained_model = model_obj\n",
    "        \n",
    "        # feature importance loggen (wenn vorhanden)\n",
    "        if hasattr(trained_model, 'feature_importances_') and feature_columns is not None:\n",
    "            feat_imp = {str(feature_columns[i]): float(trained_model.feature_importances_[i]) \n",
    "                       for i in range(len(feature_columns))}\n",
    "            mlflow.log_dict(feat_imp, \"feature_importances.json\")\n",
    "            print(\"  Feature importances logged.\")\n",
    "        \n",
    "        # model loggen und registrieren\n",
    "        mlflow.sklearn.log_model(\n",
    "            trained_model,\n",
    "            \"model\",\n",
    "            input_example=X_train_data.head(1) if hasattr(X_train_data, 'head') else X_train_data[:1],\n",
    "            registered_model_name=registered_model_name\n",
    "        )\n",
    "        print(f\"  Model registered as '{registered_model_name}'\")\n",
    "        \n",
    "        # evaluation\n",
    "        print(f\"  Evaluating {model_name}...\")\n",
    "        y_pred = trained_model.predict(X_test_data)\n",
    "        y_prob = trained_model.predict_proba(X_test_data)[:, 1]\n",
    "        \n",
    "        # metriken berechnen\n",
    "        accuracy = accuracy_score(y_test_data, y_pred)\n",
    "        precision = precision_score(y_test_data, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test_data, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test_data, y_pred, zero_division=0)\n",
    "        auc = roc_auc_score(y_test_data, y_prob)\n",
    "        \n",
    "        # weighted score (2x recall + 1x precision) / 3\n",
    "        weighted_score = (2 * recall + precision) / 3\n",
    "        \n",
    "        # metriken loggen\n",
    "        mlflow.log_metric(\"test_accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"test_precision\", precision)\n",
    "        mlflow.log_metric(\"test_recall\", recall)\n",
    "        mlflow.log_metric(\"test_f1_score\", f1)\n",
    "        mlflow.log_metric(\"test_roc_auc\", auc)\n",
    "        mlflow.log_metric(\"test_weighted_score\", weighted_score)\n",
    "        \n",
    "        print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f} <-- wichtigste Metrik!\")\n",
    "        print(f\"  F1-Score:  {f1:.4f}\")\n",
    "        print(f\"  ROC-AUC:   {auc:.4f}\")\n",
    "        print(f\"  Weighted:  {weighted_score:.4f}\")\n",
    "        \n",
    "        # plots erstellen und loggen\n",
    "        cm_path = plot_confusion_matrix(model_name, y_test_data, y_pred)\n",
    "        mlflow.log_artifact(cm_path)\n",
    "        os.remove(cm_path)\n",
    "        \n",
    "        roc_path = plot_roc_curve(model_name, y_test_data, y_prob)\n",
    "        mlflow.log_artifact(roc_path)\n",
    "        os.remove(roc_path)\n",
    "        \n",
    "        pr_path = plot_precision_recall_curve(model_name, y_test_data, y_prob)\n",
    "        mlflow.log_artifact(pr_path)\n",
    "        os.remove(pr_path)\n",
    "        \n",
    "        print(\"  Plots logged.\")\n",
    "        print(f\"--- Finished MLflow run for: {model_name} ---\")\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'recall': recall,\n",
    "            'weighted_score': weighted_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Runs ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# registered model names\n",
    "registered_model_names = {\n",
    "    \"LogisticRegression\": \"ChurnClassifier_LogisticRegression\",\n",
    "    \"RandomForest\": \"ChurnClassifier_RandomForest\",\n",
    "    \"GradientBoosting\": \"ChurnClassifier_GradientBoosting\",\n",
    "    \"KNN\": \"ChurnClassifier_KNN\",\n",
    "    \"SVC\": \"ChurnClassifier_SVC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alle runs speichern für vergleich\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base logistic regression\n",
    "result = train_evaluate_log_model(\n",
    "    log_reg, \"Base LogisticRegression\",\n",
    "    X_train_scaled_df, y_train_flat,\n",
    "    X_test_scaled_df, y_test_flat,\n",
    "    X_train_enc.columns.tolist(),\n",
    "    registered_model_names[\"LogisticRegression\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base random forest (braucht keine skalierung)\n",
    "result = train_evaluate_log_model(\n",
    "    rfc, \"Base RandomForest\",\n",
    "    X_train_enc, y_train_flat,\n",
    "    X_test_enc, y_test_flat,\n",
    "    X_train_enc.columns.tolist(),\n",
    "    registered_model_names[\"RandomForest\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base gradient boosting\n",
    "result = train_evaluate_log_model(\n",
    "    gbc, \"Base GradientBoosting\",\n",
    "    X_train_enc, y_train_flat,\n",
    "    X_test_enc, y_test_flat,\n",
    "    X_train_enc.columns.tolist(),\n",
    "    registered_model_names[\"GradientBoosting\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base knn\n",
    "result = train_evaluate_log_model(\n",
    "    knn, \"Base KNN\",\n",
    "    X_train_scaled_df, y_train_flat,\n",
    "    X_test_scaled_df, y_test_flat,\n",
    "    None,\n",
    "    registered_model_names[\"KNN\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base svc\n",
    "result = train_evaluate_log_model(\n",
    "    svc, \"Base SVC\",\n",
    "    X_train_scaled_df, y_train_flat,\n",
    "    X_test_scaled_df, y_test_flat,\n",
    "    None,\n",
    "    registered_model_names[\"SVC\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned logistic regression\n",
    "result = train_evaluate_log_model(\n",
    "    grid_log_reg, \"Tuned LogisticRegression (GridSearch)\",\n",
    "    X_train_scaled_df, y_train_flat,\n",
    "    X_test_scaled_df, y_test_flat,\n",
    "    X_train_enc.columns.tolist(),\n",
    "    registered_model_names[\"LogisticRegression\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned random forest\n",
    "result = train_evaluate_log_model(\n",
    "    grid_rfc, \"Tuned RandomForest (GridSearch)\",\n",
    "    X_train_enc, y_train_flat,\n",
    "    X_test_enc, y_test_flat,\n",
    "    X_train_enc.columns.tolist(),\n",
    "    registered_model_names[\"RandomForest\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned gradient boosting\n",
    "result = train_evaluate_log_model(\n",
    "    grid_gbc, \"Tuned GradientBoosting (GridSearch)\",\n",
    "    X_train_enc, y_train_flat,\n",
    "    X_test_enc, y_test_flat,\n",
    "    X_train_enc.columns.tolist(),\n",
    "    registered_model_names[\"GradientBoosting\"]\n",
    ")\n",
    "all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All MLflow runs complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnisse vergleichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ergebnisse als dataframe\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values('weighted_score', ascending=False)\n",
    "print(\"\\nModell-Ranking (nach Weighted Score = 2x Recall + 1x Precision):\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Champion Modell bestimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "# wir nutzen weighted_score als champion metrik (2x recall + precision)\n",
    "CHAMPION_METRIC = \"test_weighted_score\"\n",
    "GLOBAL_CHAMPION_ALIAS = \"champion\"\n",
    "\n",
    "print(f\"--- MLflow Champion Promotion ---\")\n",
    "print(f\"Champion wird basierend auf '{CHAMPION_METRIC}' ausgewählt\")\n",
    "print(f\"(Weighted Score = 2x Recall + 1x Precision, weil Recall wichtiger ist!)\")\n",
    "\n",
    "try:\n",
    "    registered_models = client.search_registered_models()\n",
    "    if not registered_models:\n",
    "        print(\"Keine registrierten Modelle gefunden.\")\n",
    "    else:\n",
    "        print(f\"\\n--- Collecting All Model Versions ---\")\n",
    "        all_versions = []\n",
    "        \n",
    "        for rm in registered_models:\n",
    "            if not rm.name.startswith(\"ChurnClassifier\"):\n",
    "                continue\n",
    "                \n",
    "            print(f\"  Processing: {rm.name}\")\n",
    "            model_versions = client.search_model_versions(f\"name='{rm.name}'\")\n",
    "            \n",
    "            for mv in model_versions:\n",
    "                try:\n",
    "                    run = client.get_run(mv.run_id)\n",
    "                    metric_value = run.data.metrics.get(CHAMPION_METRIC)\n",
    "                    \n",
    "                    if metric_value is not None:\n",
    "                        all_versions.append({\n",
    "                            \"registered_model_name\": rm.name,\n",
    "                            \"version\": mv.version,\n",
    "                            \"run_id\": mv.run_id,\n",
    "                            \"metric_value\": metric_value,\n",
    "                            \"aliases\": mv.aliases\n",
    "                        })\n",
    "                        print(f\"    Version {mv.version}: {CHAMPION_METRIC}={metric_value:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error: {e}\")\n",
    "        \n",
    "        if all_versions:\n",
    "            # bestes modell finden\n",
    "            best = max(all_versions, key=lambda x: x['metric_value'])\n",
    "            print(f\"\\n--- Champion gefunden ---\")\n",
    "            print(f\"Model: {best['registered_model_name']}\")\n",
    "            print(f\"Version: {best['version']}\")\n",
    "            print(f\"{CHAMPION_METRIC}: {best['metric_value']:.4f}\")\n",
    "            \n",
    "            # champion alias setzen\n",
    "            # erst alte aliases entfernen\n",
    "            for v in all_versions:\n",
    "                if GLOBAL_CHAMPION_ALIAS in v['aliases']:\n",
    "                    client.delete_registered_model_alias(\n",
    "                        name=v['registered_model_name'],\n",
    "                        alias=GLOBAL_CHAMPION_ALIAS\n",
    "                    )\n",
    "                    print(f\"  Removed '{GLOBAL_CHAMPION_ALIAS}' from {v['registered_model_name']} v{v['version']}\")\n",
    "            \n",
    "            # neuen champion setzen\n",
    "            client.set_registered_model_alias(\n",
    "                name=best['registered_model_name'],\n",
    "                alias=GLOBAL_CHAMPION_ALIAS,\n",
    "                version=best['version']\n",
    "            )\n",
    "            print(f\"\\n✓ Champion alias gesetzt auf {best['registered_model_name']} Version {best['version']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Champion Promotion abgeschlossen ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFertig! Check MLflow UI für alle Ergebnisse.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
