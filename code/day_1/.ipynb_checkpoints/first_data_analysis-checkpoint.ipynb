{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset: https://www.kaggle.com/datasets/devansodariya/student-performance-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../data/day_1/kaggle_mock_data.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Überprüfen der Daten auf Richtigkeit und Vollständigkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Number of rows: {data.shape[0]}\")\n",
    "print(f\"Number of columns: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = data.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = data.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if values in specified columns are in expected ranges\n",
    "grade_cols = ['G1', 'G2', 'G3']\n",
    "for col in grade_cols:\n",
    "    min_val = data[col].min()\n",
    "    max_val = data[col].max()\n",
    "    print(f\"\\nGrade {col}: Range from {min_val} to {max_val}\")\n",
    "    if min_val < 0 or max_val > 20:\n",
    "        print(f\"Warning: {col} has values outside the expected range of 0-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distribution of values in percent for each column\n",
    "distribution_percent = {}\n",
    "numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "anomalies = []  # List to store anomalies (column, value, percentage)\n",
    "\n",
    "# Handle categorical columns\n",
    "for col in categorical_cols:\n",
    "    distribution = data[col].value_counts(normalize=True) * 100\n",
    "    distribution_percent[col] = distribution\n",
    "    \n",
    "    # Check for anomalies (values with < 5% occurrence)\n",
    "    for value, percentage in distribution.items():\n",
    "        if percentage < 3.0:\n",
    "            anomalies.append((col, value, percentage))\n",
    "\n",
    "# Handle numeric columns\n",
    "for col in numeric_cols:\n",
    "    distribution = data[col].value_counts(normalize=True) * 100\n",
    "    distribution_percent[col] = distribution\n",
    "    \n",
    "    # Check for anomalies (values with < 5% occurrence)\n",
    "    for value, percentage in distribution.items():\n",
    "        if percentage < 3.0:\n",
    "            anomalies.append((col, value, percentage))\n",
    "\n",
    "# Display the distribution\n",
    "# for column, distribution in distribution_percent.items():\n",
    "#     print(f\"\\nDistribution for column '{column}':\")\n",
    "#     print(distribution)\n",
    "\n",
    "# Display anomalies\n",
    "print(\"\\nAnomalies (values with < 3% occurrence):\")\n",
    "print(f\"Total anomalies found: {len(anomalies)}\")\n",
    "print(\"Format: (column, value, percentage)\")\n",
    "for anomaly in anomalies:  # Show first 10 anomalies\n",
    "    print(anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use data[\"feature_name\"].value_counts() to investigate on strange looking features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"\\nData Quality Summary:\")\n",
    "print(f\"- Total rows: {data.shape[0]}\")\n",
    "print(f\"- Missing values: {data.isnull().any().sum()} columns with missing values\")\n",
    "print(f\"- Duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysieren der Zielvariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(set(data.G1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(set(data.G2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(set(data.G3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"G1 mean: \", data.G1.mean())\n",
    "print(\"G2 mean: \", data.G2.mean())\n",
    "print(\"G3 mean: \", data.G3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"G1 median: \", data.G1.median())\n",
    "print(\"G2 median: \", data.G2.median())\n",
    "print(\"G3 median: \", data.G3.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"G1 standard deviation: \", data.G1.std())\n",
    "print(\"G2 standard deviation: \", data.G2.std())\n",
    "print(\"G3 standard deviation: \", data.G3.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness measures the asymmetry of a distribution's probability distribution:\n",
    "\n",
    "Positive skew: When the right tail is longer (data is concentrated on the left)\n",
    "\n",
    "Zero skew: Symmetric distribution\n",
    "\n",
    "Negative skew: When the left tail is longer (data is concentrated on the right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"G1 skewness: \", data.G1.skew())\n",
    "print(\"G2 skewness: \", data.G2.skew())\n",
    "print(\"G3 skewness: \", data.G3.skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis measures the \"tailedness\" of a distribution:\n",
    "\n",
    "Higher kurtosis: Heavy tails, more outliers (leptokurtic)\n",
    "\n",
    "Normal distribution: Kurtosis of 3 (mesokurtic) --> pandas already calculates kurtosis -3 --> for pandas neutral would be a value of 0\n",
    "\n",
    "Lower kurtosis: Light tails, fewer outliers (platykurtic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"G1 kurtosis: \", data.G1.kurt())\n",
    "print(\"G2 kurtosis: \", data.G2.kurt())\n",
    "print(\"G3 kurtosis: \", data.G3.kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_counts = data['G1'].value_counts()\n",
    "g2_counts = data['G2'].value_counts()\n",
    "g3_counts = data['G3'].value_counts()\n",
    "\n",
    "print(\"G1 Value Counts:\\n\", g1_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nG2 Value Counts:\\n\", g2_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nG3 Value Counts:\\n\", g3_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysieren der Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kategorisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unique values in categorical columns\n",
    "print(\"\\nUnique values in categorical columns:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {data[col].nunique()} unique values - {sorted(data[col].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numerisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check numeric columns for outliers and distributions\n",
    "print(\"\\nStatistics for numeric columns:\")\n",
    "print(data[numeric_cols].describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Analyze categorical features\n",
    "categorical_analysis = {}\n",
    "for col in categorical_cols:\n",
    "    # Get value counts and percentages\n",
    "    counts = data[col].value_counts()\n",
    "    percentages = data[col].value_counts(normalize=True) * 100\n",
    "    categorical_analysis[col] = {\n",
    "        'counts': counts,\n",
    "        'unique_values': len(counts),\n",
    "        'most_common': counts.index[0],\n",
    "        'most_common_pct': percentages.iloc[0],\n",
    "        'least_common': counts.index[-1],\n",
    "        'least_common_pct': percentages.iloc[-1]\n",
    "    }\n",
    "\n",
    "# Display summary of categorical variables\n",
    "print(\"CATEGORICAL FEATURES ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "for col, stats in categorical_analysis.items():\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  - Unique values: {stats['unique_values']}\")\n",
    "    print(f\"  - Most common: '{stats['most_common']}' ({stats['most_common_pct']:.2f}%)\")\n",
    "    print(f\"  - Least common: '{stats['least_common']}' ({stats['least_common_pct']:.2f}%)\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Analyze numeric features\n",
    "numeric_analysis = {}\n",
    "for col in numeric_cols:\n",
    "    if col not in grade_cols:  # Exclude target variables (G1, G2, G3)\n",
    "        numeric_analysis[col] = {\n",
    "            'min': data[col].min(),\n",
    "            'max': data[col].max(),\n",
    "            'mean': data[col].mean(),\n",
    "            'median': data[col].median(),\n",
    "            'std': data[col].std(),\n",
    "            'skew': data[col].skew(),\n",
    "            'kurt': data[col].kurt(),\n",
    "            'unique_values': data[col].nunique()\n",
    "        }\n",
    "\n",
    "# Display summary of numeric variables\n",
    "print(\"\\nNUMERIC FEATURES ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "for col, stats in numeric_analysis.items():\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  - Range: {stats['min']} to {stats['max']} (span: {stats['max']-stats['min']})\")\n",
    "    print(f\"  - Central tendency: mean={stats['mean']:.2f}, median={stats['median']}\")\n",
    "    print(f\"  - Dispersion: std={stats['std']:.2f}\")\n",
    "    print(f\"  - Distribution: skewness={stats['skew']:.2f}, kurtosis={stats['kurt']:.2f}\")\n",
    "    print(f\"  - Unique values: {stats['unique_values']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative für Basis Statistiken der numerischen Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check numeric columns for outliers and distributions\n",
    "numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(\"\\nStatistics for numeric columns:\")\n",
    "print(data[numeric_cols].describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suche nach Korrelation zwischen Zielvariable und Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and targets\n",
    "targets = ['G1', 'G2', 'G3']\n",
    "\n",
    "# Convert categorical variables to numeric using one-hot encoding\n",
    "data_encoded = pd.get_dummies(data)\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_matrix = data_encoded.corr()\n",
    "\n",
    "# Extract correlations with target variables\n",
    "correlations_with_targets = {}\n",
    "for target in targets:\n",
    "    # Sort correlations by absolute value\n",
    "    correlations = correlation_matrix[target].sort_values(ascending=False)\n",
    "    correlations_with_targets[target] = correlations\n",
    "\n",
    "# Display top correlations for each target\n",
    "for target in targets:\n",
    "    print(f\"\\n=== Top 10 Correlations with {target} ===\")\n",
    "    print(correlations_with_targets[target][:10])\n",
    "    print(f\"\\n=== Bottom 10 Correlations with {target} ===\")\n",
    "    print(correlations_with_targets[target][-10:])\n",
    "\n",
    "target_corr = correlation_matrix[targets].drop(targets)\n",
    "top_corr_features = target_corr.abs().mean(axis=1).sort_values(ascending=False)[:15].index \n",
    "print(\"Top correlation features: \", top_corr_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suche nach Mustern in den Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for our analysis\n",
    "data_copy = data.copy()\n",
    "\n",
    "# Function to test feature combinations for stronger correlation than individual features\n",
    "def test_feature_combination(df, feature1, feature2, target='G3'):\n",
    "    \"\"\"\n",
    "    Test if a combination of features has stronger correlation with target\n",
    "    than the individual features alone.\n",
    "    \"\"\"\n",
    "    # Create the combined feature\n",
    "    if isinstance(feature1, tuple):\n",
    "        # If feature1 is already a condition\n",
    "        condition1 = df.eval(feature1[0])\n",
    "        name1 = feature1[1]\n",
    "    else:\n",
    "        # Create condition based on feature type\n",
    "        if df[feature1].dtype == 'object':\n",
    "            condition1 = df[feature1] == df[feature1].value_counts().index[0]\n",
    "            name1 = f\"{feature1}_{df[feature1].value_counts().index[0]}\"\n",
    "        else:\n",
    "            threshold1 = df[feature1].median()\n",
    "            condition1 = df[feature1] > threshold1\n",
    "            name1 = f\"{feature1}>{threshold1}\"\n",
    "    \n",
    "    if isinstance(feature2, tuple):\n",
    "        # If feature2 is already a condition\n",
    "        condition2 = df.eval(feature2[0])\n",
    "        name2 = feature2[1]\n",
    "    else:\n",
    "        # Create condition based on feature type\n",
    "        if df[feature2].dtype == 'object':\n",
    "            condition2 = df[feature2] == df[feature2].value_counts().index[0]\n",
    "            name2 = f\"{feature2}_{df[feature2].value_counts().index[0]}\"\n",
    "        else:\n",
    "            threshold2 = df[feature2].median()\n",
    "            condition2 = df[feature2] > threshold2\n",
    "            name2 = f\"{feature2}>{threshold2}\"\n",
    "    \n",
    "    # Create the combined feature\n",
    "    df[f\"{name1}_AND_{name2}\"] = condition1 & condition2\n",
    "    \n",
    "    # Calculate correlations\n",
    "    corr1 = df[target].corr(condition1.astype(int))\n",
    "    corr2 = df[target].corr(condition2.astype(int))\n",
    "    corr_combined = df[target].corr(df[f\"{name1}_AND_{name2}\"].astype(int))\n",
    "    \n",
    "    # Calculate if combined correlation is stronger than individual ones\n",
    "    is_stronger = abs(corr_combined) > max(abs(corr1), abs(corr2))\n",
    "    \n",
    "    # Drop the temporary column\n",
    "    df.drop(f\"{name1}_AND_{name2}\", axis=1, inplace=True)\n",
    "    \n",
    "    return {\n",
    "        'feature1': name1, \n",
    "        'feature2': name2,\n",
    "        'corr1': corr1, \n",
    "        'corr2': corr2, \n",
    "        'corr_combined': corr_combined,\n",
    "        'is_stronger': is_stronger\n",
    "    }\n",
    "\n",
    "# Define feature groups to test\n",
    "educational_features = ['studytime', 'failures', ('higher == \"yes\"', 'higher_yes')]\n",
    "family_features = ['Medu', 'Fedu', 'famrel', ('famsup == \"yes\"', 'famsup_yes')]\n",
    "lifestyle_features = [('internet == \"yes\"', 'internet_yes'), 'goout', 'Dalc', 'Walc', 'studytime']\n",
    "school_features = [('school == \"GP\"', 'school_GP'), ('address == \"U\"', 'address_U'), \n",
    "                  ('schoolsup == \"yes\"', 'schoolsup_yes'), ('higher == \"yes\"', 'higher_yes')]\n",
    "negative_indicators = ['failures', 'absences', 'Dalc', 'Walc', 'goout', 'age']\n",
    "gender_patterns = [('sex == \"F\"', 'sex_F'), ('romantic == \"no\"', 'romantic_no'), 'Dalc', 'Walc']\n",
    "\n",
    "# Test all combinations within feature groups\n",
    "feature_groups = {\n",
    "    'Educational': educational_features,\n",
    "    'Family': family_features,\n",
    "    'Lifestyle': lifestyle_features,\n",
    "    'School': school_features,\n",
    "    'Negative': negative_indicators,\n",
    "    'Gender': gender_patterns\n",
    "}\n",
    "\n",
    "# Store the results\n",
    "strong_combinations = []\n",
    "\n",
    "# Test combinations within each feature group\n",
    "for group_name, features in feature_groups.items():\n",
    "    print(f\"\\n=== Testing {group_name} Feature Combinations ===\")\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i+1, len(features)):\n",
    "            result = test_feature_combination(data_copy, features[i], features[j])\n",
    "            if result['is_stronger']:\n",
    "                strong_combinations.append(result)\n",
    "                print(f\"Strong combination found: {result['feature1']} AND {result['feature2']}\")\n",
    "                print(f\"  Individual correlations: {result['corr1']:.3f}, {result['corr2']:.3f}\")\n",
    "                print(f\"  Combined correlation: {result['corr_combined']:.3f}\")\n",
    "\n",
    "# Test selected cross-group combinations\n",
    "print(\"\\n=== Testing Cross-Group Feature Combinations ===\")\n",
    "cross_combinations = [\n",
    "    (('failures == 0', 'failures=0'), ('higher == \"yes\"', 'higher_yes')),\n",
    "    ('studytime', ('higher == \"yes\"', 'higher_yes')),\n",
    "    (('sex == \"F\"', 'sex_F'), ('romantic == \"no\"', 'romantic_no')),\n",
    "    ('Dalc', ('goout < 3', 'low_goout')),\n",
    "    ('absences', 'failures'),\n",
    "    ('Medu', 'Fedu')\n",
    "]\n",
    "\n",
    "for combo in cross_combinations:\n",
    "    result = test_feature_combination(data_copy, combo[0], combo[1])\n",
    "    if result['is_stronger']:\n",
    "        strong_combinations.append(result)\n",
    "        print(f\"Strong combination found: {result['feature1']} AND {result['feature2']}\")\n",
    "        print(f\"  Individual correlations: {result['corr1']:.3f}, {result['corr2']:.3f}\")\n",
    "        print(f\"  Combined correlation: {result['corr_combined']:.3f}\")\n",
    "\n",
    "# Test for non-linear patterns\n",
    "print(\"\\n=== Testing Non-Linear Patterns ===\")\n",
    "nonlinear_features = ['age', 'absences', 'studytime', 'goout', 'Dalc', 'Walc']\n",
    "\n",
    "for feature in nonlinear_features:\n",
    "    # Test if medium values are better than extremes\n",
    "    data_copy[f'{feature}_mid'] = (data_copy[feature] > data_copy[feature].quantile(0.25)) & \\\n",
    "                                  (data_copy[feature] < data_copy[feature].quantile(0.75))\n",
    "    \n",
    "    # Test if extremes are better than middle\n",
    "    data_copy[f'{feature}_extreme'] = (data_copy[feature] <= data_copy[feature].quantile(0.25)) | \\\n",
    "                                      (data_copy[feature] >= data_copy[feature].quantile(0.75))\n",
    "    \n",
    "    # Calculate correlations\n",
    "    corr_normal = data_copy['G3'].corr(data_copy[feature])\n",
    "    corr_mid = data_copy['G3'].corr(data_copy[f'{feature}_mid'].astype(int))\n",
    "    corr_extreme = data_copy['G3'].corr(data_copy[f'{feature}_extreme'].astype(int))\n",
    "    \n",
    "    # Check if transformation reveals stronger pattern\n",
    "    if abs(corr_mid) > abs(corr_normal) or abs(corr_extreme) > abs(corr_normal):\n",
    "        print(f\"Non-linear pattern found for {feature}:\")\n",
    "        print(f\"  Normal correlation: {corr_normal:.3f}\")\n",
    "        print(f\"  Mid-range correlation: {corr_mid:.3f}\")\n",
    "        print(f\"  Extremes correlation: {corr_extreme:.3f}\")\n",
    "        \n",
    "    # Drop temporary columns\n",
    "    data_copy.drop([f'{feature}_mid', f'{feature}_extreme'], axis=1, inplace=True)\n",
    "\n",
    "# Count subgroups with extreme performance\n",
    "print(\"\\n=== Identifying Extreme Performance Groups ===\")\n",
    "\n",
    "# Define extreme performance (top and bottom 15%)\n",
    "top_threshold = data['G3'].quantile(0.85)\n",
    "bottom_threshold = data['G3'].quantile(0.15)\n",
    "\n",
    "# Function to find subgroups with unusual performance\n",
    "def identify_extreme_groups(df, feature, target='G3'):\n",
    "    if df[feature].dtype == 'object':\n",
    "        # For categorical features\n",
    "        for category in df[feature].unique():\n",
    "            subset = df[df[feature] == category]\n",
    "            top_performers = (subset[target] >= top_threshold).mean() * 100\n",
    "            bottom_performers = (subset[target] <= bottom_threshold).mean() * 100\n",
    "            \n",
    "            # Check if this subgroup has substantially different performance\n",
    "            if top_performers > 25 or bottom_performers > 25:\n",
    "                print(f\"Extreme group found: {feature} = {category}\")\n",
    "                print(f\"  % in top performers: {top_performers:.1f}%\")\n",
    "                print(f\"  % in bottom performers: {bottom_performers:.1f}%\")\n",
    "                print(f\"  Sample size: {len(subset)}\")\n",
    "    else:\n",
    "        # For numeric features, divide into quartiles\n",
    "        for i, threshold in enumerate([0.25, 0.5, 0.75]):\n",
    "            category = f\"Q{i+1}\"\n",
    "            subset = df[df[feature] <= df[feature].quantile(threshold)] if i == 0 else \\\n",
    "                    df[(df[feature] > df[feature].quantile(threshold - 0.25)) & \n",
    "                       (df[feature] <= df[feature].quantile(threshold))]\n",
    "            if i == 3:\n",
    "                subset = df[df[feature] > df[feature].quantile(0.75)]\n",
    "                \n",
    "            top_performers = (subset[target] >= top_threshold).mean() * 100\n",
    "            bottom_performers = (subset[target] <= bottom_threshold).mean() * 100\n",
    "            \n",
    "            # Check if this subgroup has substantially different performance\n",
    "            if top_performers > 25 or bottom_performers > 25:\n",
    "                print(f\"Extreme group found: {feature} {category}\")\n",
    "                print(f\"  % in top performers: {top_performers:.1f}%\")\n",
    "                print(f\"  % in bottom performers: {bottom_performers:.1f}%\")\n",
    "                print(f\"  Sample size: {len(subset)}\")\n",
    "\n",
    "# Check important features for extreme performance groups\n",
    "important_features = ['failures', 'higher', 'school', 'studytime', 'Medu', 'Fedu', \n",
    "                      'Dalc', 'Walc', 'absences', 'sex', 'romantic']\n",
    "\n",
    "for feature in important_features:\n",
    "    identify_extreme_groups(data, feature)\n",
    "\n",
    "# Summary of findings\n",
    "print(\"\\n=== Summary of Key Patterns ===\")\n",
    "print(f\"Total strong feature combinations found: {len(strong_combinations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
